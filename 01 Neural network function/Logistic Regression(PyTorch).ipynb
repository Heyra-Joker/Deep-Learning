{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "[Pytorch](https://pytorch.org/)也是使用tensor,并且可以自动求导,定义的方式也会比TF简单一些.主要使用的是类继承的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Change data to tensor & Create. \n",
    "\n",
    "**只有tensor类型的数据才具有自动求导的能力,所以我们需要将数据转换成tensor.**并且requires_grad=True表示需要计算梯度(渐变)\n",
    "\n",
    "#### 1.1 Numpy to tensor\n",
    "我们可以将Numpy数据转换为tensor.\n",
    "```python\n",
    "torch.from_numpy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array([[1],[0],[0],[1],[0]], dtype='float32').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: torch.Size([5, 3])\n",
      "targets shape: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print('inputs shape:',inputs.shape)\n",
    "print('targets shape:',targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.typename(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样我们就实现了将Numpy数据转换为tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Crate tensor\n",
    "\n",
    "如果是需要将常数(float),向量(vector),矩阵(matrix),任意维度的数组(n-dimensional array.)转换成tensor可以使用:\n",
    "\n",
    "```python\n",
    "torch.tensor(data,requires_grad=True)```\n",
    "\n",
    "其中requires_grad=True的意思就是将元素转换成tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float_ type: torch.FloatTensor\n",
      "vector_ type: torch.DoubleTensor\n",
      "matrix type: torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "float_ = torch.tensor(1.,requires_grad=True)\n",
    "vector_ = torch.tensor(np.array([1.,2.,3.]),requires_grad=True)\n",
    "matrix = torch.tensor(np.matrix([[1.,2.],[3.,4.]]),requires_grad=True)\n",
    "print('float_ type:',torch.typename(float_))\n",
    "print('vector_ type:',torch.typename(vector_))\n",
    "print('matrix type:',torch.typename(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然,如果我们要使用正态分布的值来创建一个高维的tensor可以使用:\n",
    "```python\n",
    "torch.randn```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0266,  0.2513, -0.8446]], requires_grad=True)\n",
      "tensor([-0.0826], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn(1,3,requires_grad=True)\n",
    "b = torch.randn(1,requires_grad=True)\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Build LR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 create sigmoid model\n",
    "\n",
    "对于sigmoid函数可以使用torch.sigmoid.\n",
    "\n",
    "```python\n",
    "torch.sigmoid```\n",
    "\n",
    "对于Pytorch中:\n",
    "\n",
    "(1) 矩阵相乘:torch.mm,或者\"@\"符号\n",
    "\n",
    "(2) 矩阵转置:X.t()\n",
    "\n",
    "(3) 矩阵相加:torch.add\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X):\n",
    "    return torch.sigmoid(torch.mm(W,X.t()) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2197e-08, 1.3900e-13, 2.0821e-06, 1.8292e-08, 3.6429e-15]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define loss\n",
    "\n",
    "Pytorch可以根据定义的loss自动求导,但是需要注意的是,loss中只能有tensor,如果有其他类型的数据,tensor是不能根据链式法则求出导数的.\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "由于我们定义的loss function是带有log的,那么一旦log中的数字极为接近0或者过大\n",
    "\n",
    "那么Pytorch中的torch.log会计算出nan,且不可逆转.\n",
    "\n",
    "所以我们需要加上一个极小量来使得结果不会是nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(t1,t2):\n",
    "    epsilon = 1e-10\n",
    "    one = torch.tensor(1.,requires_grad=True)\n",
    "    part_1 = t1*torch.log(t2+epsilon)\n",
    "    part_2 = (one-t1)*torch.log(one-t2+epsilon)\n",
    "    loss_ = -torch.sum(part_1+part_2)/torch.tensor(5.,requires_grad=True)\n",
    "    return loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.0860, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = cost(targets,preds)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用\n",
    "```python\n",
    "loss.backward()```\n",
    "进行反向传播(求导)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用\n",
    "\n",
    "```python\n",
    ".grad```\n",
    "\n",
    "来查看更新的梯度(dW,db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0266,  0.2513, -0.8446]], requires_grad=True)\n",
      "tensor([[-34.8236, -21.8931, -15.9212]])\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0826], requires_grad=True)\n",
      "tensor([-0.3980])\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每次梯度更新更新之后,我们一定要设置\n",
    "\n",
    "```python\n",
    ".grad.zero_()``` \n",
    "\n",
    "将梯度重置为0.\n",
    "\n",
    "因为Pytorch在梯度计算方面是使用累加的方式,也是是说如果不重置为0,那么下次计算梯度的时候是从前次计算,而不是从当前这个值计算."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.]])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "W.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(W.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么组合在一起就是:\n",
    "\n",
    "torch.no_grad():\n",
    "\n",
    "表示不需要要将内部的代码计算渐变梯度,因为我们做的是跟新参数操作并不需要作为requires_grad=True将其归纳与tensor求导中.\n",
    "\n",
    "具体请看[torch.no_grad](https://pytorch.org/docs/stable/torch.html?highlight=torch%20no_grad)\n",
    "\n",
    "官方中文档中表示的是:\n",
    "\n",
    "如果不加torch.no_grad(),那么一个节点requires_grad被设置为True,那么所有依赖它的节点的requires_grad都为True.也就是体现需要更新操作."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[34.8501, 22.1444, 15.0766]], requires_grad=True)\n",
      "tensor([0.3154], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = cost(targets,preds)\n",
    "loss.backward()\n",
    "with torch.no_grad():\n",
    "    W -= W.grad \n",
    "    b -= b.grad \n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.8155, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = cost(targets,preds)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么我们已经可以看到W,b进行梯度变化.并且loss也改变.\n",
    "\n",
    "由于案例中$W,b$受到随机的响应,可能梯度的值会变得非常的小,导致参数更新基本无变化,这是正常现象."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据集\n",
    "这个数据集是放在h5文件中的,所以我们需要使用库h5py将图片数据读取出来."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    '''\n",
    "    create train set and test set\n",
    "    make sure you have .h5 file in your dataset\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "        train_set_x_orig: original train set shape is (209, 64, 64, 3) \n",
    "        train_set_y_orig: original train label shape is (209,)\n",
    "        test_set_x_orig: original test set shape is (50, 64, 64, 3)\n",
    "        test_set_y_orig: original test label shape is (50,)\n",
    "        classes: cat or non-cat.\n",
    "        \n",
    "        \n",
    "    Note:\n",
    "    ----\n",
    "        (209, 64, 64, 3): 209 picture,64 width,64 height,3 channel.\n",
    "    '''\n",
    "    train_dataset = h5py.File('../data_set/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('../data_set/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x's shape:(209, 12288)\n",
      "Test_x's shape:(50, 12288)\n",
      "Train_y's shape:(1, 209)\n",
      "Test_y's shape:(1, 50)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x_orig.reshape(train_x_orig.shape[0],-1) / 255 \n",
    "train_y = train_y_orig.reshape(1,-1)\n",
    "test_y  = test_y_orig.reshape(1,-1)\n",
    "test_x = test_x_orig.reshape(test_x_orig.shape[0],-1) / 255\n",
    "print('Train_x\\'s shape:{}'.format(train_x.shape))\n",
    "print('Test_x\\'s shape:{}'.format(test_x.shape))\n",
    "print(\"Train_y's shape:{}\".format(train_y.shape))\n",
    "print(\"Test_y's shape:{}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 转换数据类型\n",
    "将Numpy的数据类型转换为tensor类型."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tensor = Variable(torch.Tensor(train_x))\n",
    "test_x_tensor = Variable(torch.Tensor(test_x))\n",
    "train_y_tensor = Variable(torch.Tensor(train_y))\n",
    "test_y_tensor = Variable(torch.Tensor(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 初始化参数\n",
    "\n",
    "其中```torch.manual_seed(1)```表示设置随机数种子."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial(n):\n",
    "    \"\"\"\n",
    "    Implementation initialization parameters\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        n: fetures\n",
    "    Return:\n",
    "    ------\n",
    "        W: weights\n",
    "        b: bias\n",
    "    \"\"\"\n",
    "    torch.manual_seed(1)\n",
    "    W = torch.randn(1,n,requires_grad=True) \n",
    "    b = torch.randn(1,requires_grad=True)\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 构建pytorch LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_pytorch(X,y,learning_rate,Iter):\n",
    "    \"\"\"\n",
    "    Build Pytorch of LR.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        X: training data\n",
    "        y: training labels\n",
    "        learning_rate: learning rate\n",
    "        Iter: Iterative\n",
    "    Returns:\n",
    "    -------\n",
    "        W: best weights\n",
    "        b: best bias\n",
    "        \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    m = torch.tensor(m,requires_grad=True,dtype=torch.float32)\n",
    "    W,b = initial(n)\n",
    "    one = torch.tensor(1.,requires_grad=True)\n",
    "    one_negative = torch.tensor(-1.,requires_grad=True)\n",
    "    \n",
    "    epsilon = torch.tensor(1e-10,requires_grad=True)\n",
    "    for iter_ in range(Iter):\n",
    "        A = torch.sigmoid(torch.mm(W,X.t()) + b) # compute A\n",
    "        loss_ = -torch.sum(y*torch.log(A + epsilon)+(one-y)*torch.log(one-A + epsilon)) /m # compute loss\n",
    "        loss_.backward() # start backward propagation\n",
    "        if iter_ % 100 ==0:\n",
    "            print(\"after iter {} cost :{}\".format(iter_,loss_.item()))\n",
    "        \n",
    "        # start update\n",
    "        with torch.no_grad():\n",
    "            W -= W.grad * learning_rate\n",
    "            b -= b.grad * learning_rate\n",
    "            W.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "            \n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iter 0 cost :8.221567153930664\n",
      "after iter 100 cost :4.841607570648193\n",
      "after iter 200 cost :4.030020713806152\n",
      "after iter 300 cost :2.172579050064087\n",
      "after iter 400 cost :2.6039559841156006\n",
      "after iter 500 cost :5.026844501495361\n",
      "after iter 600 cost :1.133457899093628\n",
      "after iter 700 cost :1.0972813367843628\n",
      "after iter 800 cost :0.9333294034004211\n",
      "after iter 900 cost :0.8244034051895142\n",
      "after iter 1000 cost :0.8027365803718567\n",
      "after iter 1100 cost :0.7889816164970398\n"
     ]
    }
   ],
   "source": [
    "W_,b_ = LR_pytorch(train_x_tensor,train_y_tensor,0.1,1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到梯度是逐渐呈现下降趋势."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 correct rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,y,W,b):\n",
    "    m,n = X.shape\n",
    "    predict = torch.round(torch.sigmoid(W@X.t() + b))\n",
    "    accurate = torch.sum((predict == y)).item() / m\n",
    "    return accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data correct rate is: 0.64\n"
     ]
    }
   ],
   "source": [
    "accurate = predict(test_x_tensor,test_y_tensor,W_,b_)\n",
    "print('test data correct rate is:',accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data correct rate is: 0.9617224880382775\n"
     ]
    }
   ],
   "source": [
    "accurate = predict(train_x_tensor,train_y_tensor,W_,b_)\n",
    "print('train data correct rate is:',accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是自定义使用Pytorch来搭建LR.当然对于LR来说使用Pytorch继承的函数或类会更加方便\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch & Model\n",
    "\n",
    "对于Pytorch来说,大部分情况下构建神经网络(LR)是需要使用类的继承来搭建\n",
    "\n",
    "使用PyTorch定义线性回归模型一般分以下几步:\n",
    "\n",
    "1.构建继承类,定义forward函数.\n",
    "\n",
    "2.构建损失函数(loss)和优化器(optimizer) \n",
    "\n",
    "3.训练:\n",
    "\n",
    "3.1 (forward)\n",
    "\n",
    "3.2 (backward)\n",
    "\n",
    "3.3 (update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 torch.nn.Module\n",
    "我们需要先继承[torch.nn.Module](https://pytorch.org/docs/stable/nn.html?highlight=torch%20nn%20module#module-torch.nn),里面包含了许多已经集成好的函数.\n",
    "\n",
    "由于我们现在做的是LR,所以同样需要线性函数以及sigmoid函数:\n",
    "\n",
    "(1) [torch.nn.Linear(in_features, out_features,...)](https://pytorch.org/docs/stable/nn.html#linear-functions)\n",
    "\n",
    "in_features:线性输入的特征数量\n",
    "\n",
    "out_features:线性结果输出的特征\n",
    "\n",
    "具体使用方法请查看函数帮助\n",
    "\n",
    "(2) [torch.sigmoid](https://pytorch.org/docs/stable/torch.html?highlight=torch%20sigmoid#torch.sigmoid)\n",
    "\n",
    "torch中的sigmoid函数\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "我们只需要在class中定义forward函数,将向前传播的步骤一一写下就行."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_regression(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Create LR class.\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,n,m):\n",
    "        torch.nn.Module.__init__(self)\n",
    "        self.linear = torch.nn.Linear(n,1)\n",
    "        \n",
    "    def forward(self,X):\n",
    "       \n",
    "        return torch.sigmoid(self.linear(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 build model\n",
    "\n",
    "当继承torch的类定义完毕且函数forward也定义完毕就可以开始构建模型.\n",
    "\n",
    "(1) 定义损失函数ceriterion,这里使用的是[torch.nn.BCELoss](https://pytorch.org/docs/stable/nn.html?highlight=torch%20nn%20bceloss#torch.nn.BCELoss)也就是Binary Cross Entropy.\n",
    "\n",
    "(2) 定义optimizer,这里使用的是[torch.optim.SGD](https://pytorch.org/docs/stable/optim.html?highlight=torch%20optim%20sgd)\n",
    "\n",
    "(3) 开始迭代:\n",
    "\n",
    "(3.1) 使用定义好的类进行预测\n",
    "\n",
    "(3.2) 计算loss\n",
    "\n",
    "(3.3) 需要先将optimizer重置为0,因为Pytorch是使用累加渐变梯度的.\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "(3.4) 反向传播计算梯度loss.backward().\n",
    "\n",
    "(3.5) 更新所有参数[optimizer.step()](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-optim/)\n",
    "\n",
    "(4) 返回我们的线性模型,从中获取参数:\n",
    "\n",
    "(4.1) 第一种直接返回定义类中的parameters(),它是一个迭代器,需要使用循环将其拿出来.\n",
    "\n",
    "(4.2) 第二种直接返回线性模型,使用.weight.data或者.bias.data 获取参数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_pytorch(X,y,test_X,test_y,alpha,Iter):\n",
    "    \n",
    "    m,n = X.shape\n",
    "    \n",
    "    # define model\n",
    "    model = Logistic_regression(n,m)\n",
    "    \n",
    "    # define loss ceriterion\n",
    "    ceriterion = torch.nn.BCELoss(reduction='mean')\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=alpha)\n",
    "    \n",
    "    # train loop\n",
    "    for iter_ in range(Iter):\n",
    "        # forward propagation\n",
    "        # predict y\n",
    "        y_predict = model(X)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = ceriterion(y_predict.t(),y)\n",
    "        if iter_ % 100 == 0:\n",
    "            print('after iter {} cost {}'.format(iter_,loss))\n",
    "            \n",
    "        # star backword propagation\n",
    "        # clear optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # backword propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    # compute correct rate\n",
    "    \n",
    "    predict = torch.round(model.forward(test_X))\n",
    "    accuracy = (predict.t() == test_y).sum().item() / test_y.shape[1]\n",
    "    print('-----------accuracy------------')\n",
    "    print(\"The test correct rate is {}\".format(accuracy))\n",
    "    \n",
    "    predict = torch.round(model.forward(X))\n",
    "    accuracy = (predict.t() == y).sum().item() / y.shape[1]\n",
    "    print(\"The train correct rate is {}\".format(accuracy))\n",
    "    \n",
    "    return model.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iter 0 cost 0.730829119682312\n",
      "after iter 100 cost 2.446681022644043\n",
      "after iter 200 cost 0.6464812159538269\n",
      "after iter 300 cost 1.020178198814392\n",
      "after iter 400 cost 0.7230757474899292\n",
      "after iter 500 cost 0.46212637424468994\n",
      "after iter 600 cost 0.2490411400794983\n",
      "after iter 700 cost 0.154551163315773\n",
      "after iter 800 cost 0.1355052888393402\n",
      "after iter 900 cost 0.12510743737220764\n",
      "-----------accuracy------------\n",
      "The test correct rate is 0.68\n",
      "The train correct rate is 0.9856459330143541\n"
     ]
    }
   ],
   "source": [
    "parameters = lr_pytorch(train_x_tensor,train_y_tensor,test_x_tensor,test_y_tensor,0.01,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出最终的结果和之前都是相似的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们拿出参数$W,b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0138, -0.0284, -0.0065,  ..., -0.0102, -0.0351,  0.0193]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0088])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Pytorch 总体而言是比TF方便的,这也是为什么Pytorch的使用人数会逐渐逼近TF(就目前而言),但是我在编写的过程中也发现了某些地方Pytorch不足的地方.不过问题也不大,随着版本的更新,会逐渐变好."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
