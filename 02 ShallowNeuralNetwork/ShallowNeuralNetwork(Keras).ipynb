{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShallowNeuralNetwork(Keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Input\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData_iris():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    --------\n",
    "        X:have two dimensions (sepal length and width).\n",
    "        Y:labels.\n",
    "    \"\"\"\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data[:100, :2]\n",
    "    Y = iris.target[:100]\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is: (100, 2)\n",
      "y shape is: (100,)\n"
     ]
    }
   ],
   "source": [
    "X,y = loadData_iris()\n",
    "print('X shape is:',X.shape)\n",
    "print('y shape is:',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (80, 2)\n",
      "X_test shape: (20, 2)\n",
      "y_train shape: (80, 1)\n",
      "y_test shape: (20, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print('X_train shape:',X_train.shape)\n",
    "print('X_test shape:',X_test.shape)\n",
    "print('y_train shape:',y_train.shape)\n",
    "print('y_test shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Sequential  Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Sequential Model建立SNN\n",
    "\n",
    "(1) 初始化Sequential\n",
    "\n",
    "```python\n",
    "model = Sequential()```\n",
    "\n",
    "(2) layer-1\n",
    "\n",
    "units=4,output shape is (m,4),so input shape (\\*,n)\n",
    "\n",
    "activation:Relu\n",
    "\n",
    "```python\n",
    "model.add(Dense(units=4,input_shape=(n,),activation='relu'))```\n",
    "\n",
    "(3) layer-2\n",
    "\n",
    "units=4,output shape is (m,1)\n",
    "\n",
    "activation:sigmoid\n",
    "\n",
    "```python\n",
    "model.add(Dense(units=1,activation='sigmoid'))```\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "当layer-1定义输入特征了之后,其他层无需定义维度\n",
    "\n",
    "(4) compile model\n",
    "\n",
    "optimizer:SGD,learning rate equal 0.1\n",
    "\n",
    "loss: binary crossentropy\n",
    "\n",
    "```python\n",
    "model.compile(...)```\n",
    "\n",
    "(5) fit model\n",
    "\n",
    "```python\n",
    "model.fit```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNN_Sequential(X,y,alpha,Iters):\n",
    "    \"\"\"\n",
    "    Build SNN of Kears sequential\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        X: training data\n",
    "        y: training labels\n",
    "        alpha: learning rate\n",
    "        Iters: Iterative number.\n",
    "    Return:\n",
    "    ------\n",
    "        model: training okay model.\n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=4,input_shape=(n,),activation='relu'))\n",
    "    model.add(Dense(units=1,activation='sigmoid'))\n",
    "    model.compile(optimizer=keras.optimizers.SGD(lr=0.1),loss=keras.losses.binary_crossentropy,metrics=['accuracy'])\n",
    "    model.fit(x=X,y=y,epochs=Iters)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 2.6089 - acc: 0.5250\n",
      "Epoch 2/300\n",
      "80/80 [==============================] - 0s 81us/step - loss: 0.7224 - acc: 0.5250\n",
      "Epoch 3/300\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.6865 - acc: 0.5500\n",
      "Epoch 4/300\n",
      "80/80 [==============================] - 0s 91us/step - loss: 0.6989 - acc: 0.5250\n",
      "Epoch 5/300\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.6880 - acc: 0.4750\n",
      "Epoch 6/300\n",
      "80/80 [==============================] - 0s 111us/step - loss: 0.7011 - acc: 0.5250\n",
      "Epoch 7/300\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.6780 - acc: 0.5125\n",
      "Epoch 8/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.6819 - acc: 0.7000\n",
      "Epoch 9/300\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.6861 - acc: 0.5250\n",
      "Epoch 10/300\n",
      "80/80 [==============================] - 0s 85us/step - loss: 0.6693 - acc: 0.6750\n",
      "Epoch 11/300\n",
      "80/80 [==============================] - 0s 91us/step - loss: 0.6681 - acc: 0.5250\n",
      "Epoch 12/300\n",
      "80/80 [==============================] - 0s 124us/step - loss: 0.6781 - acc: 0.5500\n",
      "Epoch 13/300\n",
      "80/80 [==============================] - 0s 97us/step - loss: 0.6804 - acc: 0.7250\n",
      "Epoch 14/300\n",
      "80/80 [==============================] - 0s 104us/step - loss: 0.6528 - acc: 0.6875\n",
      "Epoch 15/300\n",
      "80/80 [==============================] - 0s 102us/step - loss: 0.6538 - acc: 0.5250\n",
      "Epoch 16/300\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.6379 - acc: 0.5250\n",
      "Epoch 17/300\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.6474 - acc: 0.7000\n",
      "Epoch 18/300\n",
      "80/80 [==============================] - 0s 91us/step - loss: 0.6537 - acc: 0.5125\n",
      "Epoch 19/300\n",
      "80/80 [==============================] - 0s 121us/step - loss: 0.6567 - acc: 0.5250\n",
      "Epoch 20/300\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.6089 - acc: 0.7250\n",
      "Epoch 21/300\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.6001 - acc: 0.6250\n",
      "Epoch 22/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.5843 - acc: 0.7500\n",
      "Epoch 23/300\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.5903 - acc: 0.6750\n",
      "Epoch 24/300\n",
      "80/80 [==============================] - 0s 49us/step - loss: 0.5680 - acc: 0.8000\n",
      "Epoch 25/300\n",
      "80/80 [==============================] - 0s 108us/step - loss: 0.5550 - acc: 0.9000\n",
      "Epoch 26/300\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.5293 - acc: 0.9125\n",
      "Epoch 27/300\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.5398 - acc: 0.7875\n",
      "Epoch 28/300\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.4928 - acc: 0.9625\n",
      "Epoch 29/300\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.5079 - acc: 0.7625\n",
      "Epoch 30/300\n",
      "80/80 [==============================] - 0s 81us/step - loss: 0.4596 - acc: 0.9750\n",
      "Epoch 31/300\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.4691 - acc: 0.9125\n",
      "Epoch 32/300\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.4195 - acc: 0.9875\n",
      "Epoch 33/300\n",
      "80/80 [==============================] - 0s 108us/step - loss: 0.4816 - acc: 0.7375\n",
      "Epoch 34/300\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.3872 - acc: 0.9875\n",
      "Epoch 35/300\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.3920 - acc: 0.9000\n",
      "Epoch 36/300\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.3488 - acc: 0.9750\n",
      "Epoch 37/300\n",
      "80/80 [==============================] - 0s 104us/step - loss: 0.3373 - acc: 0.9750\n",
      "Epoch 38/300\n",
      "80/80 [==============================] - 0s 110us/step - loss: 0.3192 - acc: 0.9875\n",
      "Epoch 39/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.4036 - acc: 0.8500\n",
      "Epoch 40/300\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.3164 - acc: 0.9500\n",
      "Epoch 41/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.4198 - acc: 0.7750\n",
      "Epoch 42/300\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.3340 - acc: 0.9000\n",
      "Epoch 43/300\n",
      "80/80 [==============================] - 0s 111us/step - loss: 0.3064 - acc: 0.8875\n",
      "Epoch 44/300\n",
      "80/80 [==============================] - 0s 56us/step - loss: 0.2485 - acc: 0.9875\n",
      "Epoch 45/300\n",
      "80/80 [==============================] - 0s 106us/step - loss: 0.2359 - acc: 0.9750\n",
      "Epoch 46/300\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.2242 - acc: 0.9750\n",
      "Epoch 47/300\n",
      "80/80 [==============================] - 0s 106us/step - loss: 0.2122 - acc: 0.9875\n",
      "Epoch 48/300\n",
      "80/80 [==============================] - 0s 113us/step - loss: 0.3597 - acc: 0.8375\n",
      "Epoch 49/300\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.2116 - acc: 0.9500\n",
      "Epoch 50/300\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.1891 - acc: 0.9875\n",
      "Epoch 51/300\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.1844 - acc: 0.9750\n",
      "Epoch 52/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.2080 - acc: 0.9625\n",
      "Epoch 53/300\n",
      "80/80 [==============================] - 0s 139us/step - loss: 0.2585 - acc: 0.9000\n",
      "Epoch 54/300\n",
      "80/80 [==============================] - 0s 91us/step - loss: 0.1822 - acc: 0.9750\n",
      "Epoch 55/300\n",
      "80/80 [==============================] - 0s 83us/step - loss: 0.1561 - acc: 0.9750\n",
      "Epoch 56/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.1652 - acc: 0.9625\n",
      "Epoch 57/300\n",
      "80/80 [==============================] - 0s 104us/step - loss: 0.2116 - acc: 0.9250\n",
      "Epoch 58/300\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.3217 - acc: 0.8500\n",
      "Epoch 59/300\n",
      "80/80 [==============================] - 0s 104us/step - loss: 0.1571 - acc: 0.9750\n",
      "Epoch 60/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.1776 - acc: 0.9625\n",
      "Epoch 61/300\n",
      "80/80 [==============================] - 0s 91us/step - loss: 0.2521 - acc: 0.8750\n",
      "Epoch 62/300\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.1762 - acc: 0.9250\n",
      "Epoch 63/300\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.1281 - acc: 0.9875\n",
      "Epoch 64/300\n",
      "80/80 [==============================] - 0s 109us/step - loss: 0.1245 - acc: 0.9875\n",
      "Epoch 65/300\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.1456 - acc: 0.9750\n",
      "Epoch 66/300\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.1361 - acc: 0.9750\n",
      "Epoch 67/300\n",
      "80/80 [==============================] - 0s 90us/step - loss: 0.1208 - acc: 0.9750\n",
      "Epoch 68/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.1148 - acc: 0.9875\n",
      "Epoch 69/300\n",
      "80/80 [==============================] - 0s 105us/step - loss: 0.1317 - acc: 0.9875\n",
      "Epoch 70/300\n",
      "80/80 [==============================] - 0s 97us/step - loss: 0.1118 - acc: 0.9875\n",
      "Epoch 71/300\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.1093 - acc: 0.9875\n",
      "Epoch 72/300\n",
      "80/80 [==============================] - 0s 164us/step - loss: 0.1055 - acc: 0.9875\n",
      "Epoch 73/300\n",
      "80/80 [==============================] - 0s 126us/step - loss: 0.1398 - acc: 0.9625\n",
      "Epoch 74/300\n",
      "80/80 [==============================] - 0s 142us/step - loss: 0.1032 - acc: 0.9750\n",
      "Epoch 75/300\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.1210 - acc: 0.9750\n",
      "Epoch 76/300\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.1189 - acc: 0.9750\n",
      "Epoch 77/300\n",
      "80/80 [==============================] - 0s 127us/step - loss: 0.1055 - acc: 0.9750\n",
      "Epoch 78/300\n",
      "80/80 [==============================] - 0s 93us/step - loss: 0.1009 - acc: 0.9750\n",
      "Epoch 79/300\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.1563 - acc: 0.9625\n",
      "Epoch 80/300\n",
      "80/80 [==============================] - 0s 129us/step - loss: 0.0948 - acc: 0.9875\n",
      "Epoch 81/300\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.0965 - acc: 0.9875\n",
      "Epoch 82/300\n",
      "80/80 [==============================] - 0s 85us/step - loss: 0.0912 - acc: 0.9875\n",
      "Epoch 83/300\n",
      "80/80 [==============================] - 0s 85us/step - loss: 0.1106 - acc: 0.9625\n",
      "Epoch 84/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.1333 - acc: 0.9500\n",
      "Epoch 85/300\n",
      "80/80 [==============================] - 0s 80us/step - loss: 0.1040 - acc: 0.9750\n",
      "Epoch 86/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 96us/step - loss: 0.0911 - acc: 0.9875\n",
      "Epoch 87/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.0938 - acc: 0.9750\n",
      "Epoch 88/300\n",
      "80/80 [==============================] - 0s 80us/step - loss: 0.1076 - acc: 0.9750\n",
      "Epoch 89/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0863 - acc: 0.9875\n",
      "Epoch 90/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0847 - acc: 0.9875\n",
      "Epoch 91/300\n",
      "80/80 [==============================] - 0s 141us/step - loss: 0.0941 - acc: 0.9875\n",
      "Epoch 92/300\n",
      "80/80 [==============================] - 0s 113us/step - loss: 0.0950 - acc: 0.9875\n",
      "Epoch 93/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.2170 - acc: 0.9250\n",
      "Epoch 94/300\n",
      "80/80 [==============================] - 0s 109us/step - loss: 0.0807 - acc: 0.9875\n",
      "Epoch 95/300\n",
      "80/80 [==============================] - 0s 89us/step - loss: 0.0799 - acc: 0.9875\n",
      "Epoch 96/300\n",
      "80/80 [==============================] - 0s 128us/step - loss: 0.0834 - acc: 0.9875\n",
      "Epoch 97/300\n",
      "80/80 [==============================] - 0s 95us/step - loss: 0.0958 - acc: 0.9750\n",
      "Epoch 98/300\n",
      "80/80 [==============================] - 0s 98us/step - loss: 0.1099 - acc: 0.9625\n",
      "Epoch 99/300\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0793 - acc: 0.9875\n",
      "Epoch 100/300\n",
      "80/80 [==============================] - 0s 96us/step - loss: 0.0857 - acc: 0.9875\n",
      "Epoch 101/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.0924 - acc: 0.9875\n",
      "Epoch 102/300\n",
      "80/80 [==============================] - 0s 91us/step - loss: 0.0897 - acc: 0.9625\n",
      "Epoch 103/300\n",
      "80/80 [==============================] - 0s 114us/step - loss: 0.0771 - acc: 0.9875\n",
      "Epoch 104/300\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0852 - acc: 0.9875\n",
      "Epoch 105/300\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.1112 - acc: 0.9625\n",
      "Epoch 106/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.0873 - acc: 0.9750\n",
      "Epoch 107/300\n",
      "80/80 [==============================] - 0s 126us/step - loss: 0.0772 - acc: 0.9875\n",
      "Epoch 108/300\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0751 - acc: 0.9875\n",
      "Epoch 109/300\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.0740 - acc: 0.9875\n",
      "Epoch 110/300\n",
      "80/80 [==============================] - 0s 95us/step - loss: 0.0924 - acc: 0.9500\n",
      "Epoch 111/300\n",
      "80/80 [==============================] - 0s 109us/step - loss: 0.0750 - acc: 0.9875\n",
      "Epoch 112/300\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.0935 - acc: 0.9750\n",
      "Epoch 113/300\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0840 - acc: 0.9625\n",
      "Epoch 114/300\n",
      "80/80 [==============================] - 0s 120us/step - loss: 0.0876 - acc: 0.9875\n",
      "Epoch 115/300\n",
      "80/80 [==============================] - 0s 81us/step - loss: 0.0709 - acc: 0.9875\n",
      "Epoch 116/300\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.0736 - acc: 0.9875\n",
      "Epoch 117/300\n",
      "80/80 [==============================] - 0s 85us/step - loss: 0.0730 - acc: 0.9875\n",
      "Epoch 118/300\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0701 - acc: 0.9875\n",
      "Epoch 119/300\n",
      "80/80 [==============================] - 0s 89us/step - loss: 0.0720 - acc: 0.9875\n",
      "Epoch 120/300\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.0703 - acc: 0.9875\n",
      "Epoch 121/300\n",
      "80/80 [==============================] - 0s 92us/step - loss: 0.0722 - acc: 0.9875\n",
      "Epoch 122/300\n",
      "80/80 [==============================] - 0s 80us/step - loss: 0.0695 - acc: 0.9750\n",
      "Epoch 123/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0677 - acc: 0.9875\n",
      "Epoch 124/300\n",
      "80/80 [==============================] - 0s 136us/step - loss: 0.0825 - acc: 0.9875\n",
      "Epoch 125/300\n",
      "80/80 [==============================] - 0s 92us/step - loss: 0.0731 - acc: 0.9875\n",
      "Epoch 126/300\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0735 - acc: 0.9875\n",
      "Epoch 127/300\n",
      "80/80 [==============================] - 0s 105us/step - loss: 0.0679 - acc: 0.9875\n",
      "Epoch 128/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0691 - acc: 0.9750\n",
      "Epoch 129/300\n",
      "80/80 [==============================] - 0s 119us/step - loss: 0.0680 - acc: 0.9875\n",
      "Epoch 130/300\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.1025 - acc: 0.9625\n",
      "Epoch 131/300\n",
      "80/80 [==============================] - 0s 110us/step - loss: 0.0696 - acc: 0.9875\n",
      "Epoch 132/300\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.0672 - acc: 0.9875\n",
      "Epoch 133/300\n",
      "80/80 [==============================] - 0s 85us/step - loss: 0.0727 - acc: 0.9875\n",
      "Epoch 134/300\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0857 - acc: 0.9625\n",
      "Epoch 135/300\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0938 - acc: 0.9750\n",
      "Epoch 136/300\n",
      "80/80 [==============================] - 0s 96us/step - loss: 0.0741 - acc: 0.9625\n",
      "Epoch 137/300\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.0649 - acc: 0.9875\n",
      "Epoch 138/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.0999 - acc: 0.9625\n",
      "Epoch 139/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.1411 - acc: 0.9625\n",
      "Epoch 140/300\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0755 - acc: 0.9750\n",
      "Epoch 141/300\n",
      "80/80 [==============================] - 0s 101us/step - loss: 0.0667 - acc: 0.9875\n",
      "Epoch 142/300\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0727 - acc: 0.9875\n",
      "Epoch 143/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.0640 - acc: 0.9875\n",
      "Epoch 144/300\n",
      "80/80 [==============================] - 0s 93us/step - loss: 0.0651 - acc: 0.9750\n",
      "Epoch 145/300\n",
      "80/80 [==============================] - 0s 96us/step - loss: 0.0726 - acc: 0.9875\n",
      "Epoch 146/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.0908 - acc: 0.9625\n",
      "Epoch 147/300\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.0812 - acc: 0.9875\n",
      "Epoch 148/300\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0796 - acc: 0.9750\n",
      "Epoch 149/300\n",
      "80/80 [==============================] - 0s 93us/step - loss: 0.0656 - acc: 0.9875\n",
      "Epoch 150/300\n",
      "80/80 [==============================] - 0s 90us/step - loss: 0.0966 - acc: 0.9625\n",
      "Epoch 151/300\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0628 - acc: 0.9875\n",
      "Epoch 152/300\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.0630 - acc: 0.9875\n",
      "Epoch 153/300\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0627 - acc: 0.9875\n",
      "Epoch 154/300\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0665 - acc: 0.9875\n",
      "Epoch 155/300\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.0641 - acc: 0.9875\n",
      "Epoch 156/300\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0682 - acc: 0.9875\n",
      "Epoch 157/300\n",
      "80/80 [==============================] - 0s 89us/step - loss: 0.0658 - acc: 0.9750\n",
      "Epoch 158/300\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0719 - acc: 0.9750\n",
      "Epoch 159/300\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.0644 - acc: 0.9875\n",
      "Epoch 160/300\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0878 - acc: 0.9750\n",
      "Epoch 161/300\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0900 - acc: 0.9625\n",
      "Epoch 162/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0630 - acc: 0.9875\n",
      "Epoch 163/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.0610 - acc: 0.9875\n",
      "Epoch 164/300\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0730 - acc: 0.9750\n",
      "Epoch 165/300\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.0604 - acc: 0.9875\n",
      "Epoch 166/300\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.0598 - acc: 0.9875\n",
      "Epoch 167/300\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0597 - acc: 0.9875\n",
      "Epoch 168/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.0606 - acc: 0.9875\n",
      "Epoch 169/300\n",
      "80/80 [==============================] - 0s 98us/step - loss: 0.0625 - acc: 0.9750\n",
      "Epoch 170/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 62us/step - loss: 0.0642 - acc: 0.9875\n",
      "Epoch 171/300\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0685 - acc: 0.9875\n",
      "Epoch 172/300\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0591 - acc: 0.9875\n",
      "Epoch 173/300\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.0632 - acc: 0.9875\n",
      "Epoch 174/300\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.0710 - acc: 0.9750\n",
      "Epoch 175/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0650 - acc: 0.9750\n",
      "Epoch 176/300\n",
      "80/80 [==============================] - 0s 104us/step - loss: 0.0818 - acc: 0.9750\n",
      "Epoch 177/300\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0607 - acc: 0.9875\n",
      "Epoch 178/300\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.0722 - acc: 0.9750\n",
      "Epoch 179/300\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.0600 - acc: 0.9750\n",
      "Epoch 180/300\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.0619 - acc: 0.9875\n",
      "Epoch 181/300\n",
      "80/80 [==============================] - 0s 93us/step - loss: 0.0588 - acc: 0.9875\n",
      "Epoch 182/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0687 - acc: 0.9875\n",
      "Epoch 183/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.0697 - acc: 0.9875\n",
      "Epoch 184/300\n",
      "80/80 [==============================] - 0s 97us/step - loss: 0.0587 - acc: 0.9875\n",
      "Epoch 185/300\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0684 - acc: 0.9750\n",
      "Epoch 186/300\n",
      "80/80 [==============================] - 0s 98us/step - loss: 0.0766 - acc: 0.9750\n",
      "Epoch 187/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.0996 - acc: 0.9625\n",
      "Epoch 188/300\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0642 - acc: 0.9750\n",
      "Epoch 189/300\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.0829 - acc: 0.9750\n",
      "Epoch 190/300\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0711 - acc: 0.9750\n",
      "Epoch 191/300\n",
      "80/80 [==============================] - 0s 147us/step - loss: 0.0615 - acc: 0.9875\n",
      "Epoch 192/300\n",
      "80/80 [==============================] - 0s 106us/step - loss: 0.0593 - acc: 0.9875\n",
      "Epoch 193/300\n",
      "80/80 [==============================] - 0s 138us/step - loss: 0.0616 - acc: 0.9875\n",
      "Epoch 194/300\n",
      "80/80 [==============================] - 0s 109us/step - loss: 0.0581 - acc: 0.9875\n",
      "Epoch 195/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0619 - acc: 0.9875\n",
      "Epoch 196/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.0673 - acc: 0.9750\n",
      "Epoch 197/300\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0572 - acc: 0.9875\n",
      "Epoch 198/300\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.0587 - acc: 0.9875\n",
      "Epoch 199/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.0583 - acc: 0.9875\n",
      "Epoch 200/300\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.0646 - acc: 0.9875\n",
      "Epoch 201/300\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.0768 - acc: 0.9875\n",
      "Epoch 202/300\n",
      "80/80 [==============================] - 0s 83us/step - loss: 0.0657 - acc: 0.9750\n",
      "Epoch 203/300\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0786 - acc: 0.9875\n",
      "Epoch 204/300\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0792 - acc: 0.9875\n",
      "Epoch 205/300\n",
      "80/80 [==============================] - 0s 60us/step - loss: 0.0605 - acc: 0.9875\n",
      "Epoch 206/300\n",
      "80/80 [==============================] - 0s 121us/step - loss: 0.0762 - acc: 0.9750\n",
      "Epoch 207/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0588 - acc: 0.9875\n",
      "Epoch 208/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.0776 - acc: 0.9750\n",
      "Epoch 209/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.0929 - acc: 0.9625\n",
      "Epoch 210/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0561 - acc: 0.9875\n",
      "Epoch 211/300\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.0633 - acc: 0.9875\n",
      "Epoch 212/300\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.1079 - acc: 0.9625\n",
      "Epoch 213/300\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0623 - acc: 0.9750\n",
      "Epoch 214/300\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0645 - acc: 0.9875\n",
      "Epoch 215/300\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0870 - acc: 0.9625\n",
      "Epoch 216/300\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0796 - acc: 0.9750\n",
      "Epoch 217/300\n",
      "80/80 [==============================] - 0s 108us/step - loss: 0.0629 - acc: 0.9875\n",
      "Epoch 218/300\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.0753 - acc: 0.9750\n",
      "Epoch 219/300\n",
      "80/80 [==============================] - 0s 102us/step - loss: 0.0828 - acc: 0.9875\n",
      "Epoch 220/300\n",
      "80/80 [==============================] - 0s 92us/step - loss: 0.0571 - acc: 0.9875\n",
      "Epoch 221/300\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0582 - acc: 0.9875\n",
      "Epoch 222/300\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0630 - acc: 0.9875\n",
      "Epoch 223/300\n",
      "80/80 [==============================] - 0s 106us/step - loss: 0.0705 - acc: 0.9875\n",
      "Epoch 224/300\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.0654 - acc: 0.9750\n",
      "Epoch 225/300\n",
      "80/80 [==============================] - 0s 49us/step - loss: 0.0759 - acc: 0.9625\n",
      "Epoch 226/300\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0613 - acc: 0.9875\n",
      "Epoch 227/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.0542 - acc: 0.9875\n",
      "Epoch 228/300\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.0561 - acc: 0.9875\n",
      "Epoch 229/300\n",
      "80/80 [==============================] - 0s 108us/step - loss: 0.0602 - acc: 0.9750\n",
      "Epoch 230/300\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.0584 - acc: 0.9875\n",
      "Epoch 231/300\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0656 - acc: 0.9750\n",
      "Epoch 232/300\n",
      "80/80 [==============================] - 0s 109us/step - loss: 0.0540 - acc: 0.9875\n",
      "Epoch 233/300\n",
      "80/80 [==============================] - 0s 93us/step - loss: 0.0575 - acc: 0.9875\n",
      "Epoch 234/300\n",
      "80/80 [==============================] - 0s 85us/step - loss: 0.0633 - acc: 0.9875\n",
      "Epoch 235/300\n",
      "80/80 [==============================] - 0s 101us/step - loss: 0.0853 - acc: 0.9750\n",
      "Epoch 236/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.0553 - acc: 0.9875\n",
      "Epoch 237/300\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.0653 - acc: 0.9875\n",
      "Epoch 238/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.0684 - acc: 0.9875\n",
      "Epoch 239/300\n",
      "80/80 [==============================] - 0s 83us/step - loss: 0.0716 - acc: 0.9750\n",
      "Epoch 240/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.0645 - acc: 0.9875\n",
      "Epoch 241/300\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0580 - acc: 0.9875\n",
      "Epoch 242/300\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0564 - acc: 0.9875\n",
      "Epoch 243/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.0733 - acc: 0.9625\n",
      "Epoch 244/300\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.0569 - acc: 0.9875\n",
      "Epoch 245/300\n",
      "80/80 [==============================] - 0s 81us/step - loss: 0.0601 - acc: 0.9750\n",
      "Epoch 246/300\n",
      "80/80 [==============================] - 0s 121us/step - loss: 0.0637 - acc: 0.9875\n",
      "Epoch 247/300\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.0615 - acc: 0.9875\n",
      "Epoch 248/300\n",
      "80/80 [==============================] - 0s 85us/step - loss: 0.0552 - acc: 0.9875\n",
      "Epoch 249/300\n",
      "80/80 [==============================] - 0s 98us/step - loss: 0.0566 - acc: 0.9875\n",
      "Epoch 250/300\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0591 - acc: 0.9875\n",
      "Epoch 251/300\n",
      "80/80 [==============================] - 0s 93us/step - loss: 0.0566 - acc: 0.9875\n",
      "Epoch 252/300\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.0823 - acc: 0.9750\n",
      "Epoch 253/300\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0557 - acc: 0.9875\n",
      "Epoch 254/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 81us/step - loss: 0.0599 - acc: 0.9750\n",
      "Epoch 255/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.0623 - acc: 0.9875\n",
      "Epoch 256/300\n",
      "80/80 [==============================] - 0s 91us/step - loss: 0.0559 - acc: 0.9875\n",
      "Epoch 257/300\n",
      "80/80 [==============================] - 0s 83us/step - loss: 0.0564 - acc: 0.9875\n",
      "Epoch 258/300\n",
      "80/80 [==============================] - 0s 106us/step - loss: 0.0605 - acc: 0.9875\n",
      "Epoch 259/300\n",
      "80/80 [==============================] - 0s 92us/step - loss: 0.0608 - acc: 0.9750\n",
      "Epoch 260/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.0539 - acc: 0.9875\n",
      "Epoch 261/300\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.0547 - acc: 0.9875\n",
      "Epoch 262/300\n",
      "80/80 [==============================] - 0s 85us/step - loss: 0.0549 - acc: 0.9750\n",
      "Epoch 263/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.0576 - acc: 0.9875\n",
      "Epoch 264/300\n",
      "80/80 [==============================] - 0s 98us/step - loss: 0.0705 - acc: 0.9750\n",
      "Epoch 265/300\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.0602 - acc: 0.9875\n",
      "Epoch 266/300\n",
      "80/80 [==============================] - 0s 83us/step - loss: 0.0587 - acc: 0.9875\n",
      "Epoch 267/300\n",
      "80/80 [==============================] - 0s 110us/step - loss: 0.0652 - acc: 0.9750\n",
      "Epoch 268/300\n",
      "80/80 [==============================] - 0s 97us/step - loss: 0.0647 - acc: 0.9875\n",
      "Epoch 269/300\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.0699 - acc: 0.9875\n",
      "Epoch 270/300\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.0699 - acc: 0.9625\n",
      "Epoch 271/300\n",
      "80/80 [==============================] - 0s 108us/step - loss: 0.0610 - acc: 0.9625\n",
      "Epoch 272/300\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.0611 - acc: 0.9875\n",
      "Epoch 273/300\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.0663 - acc: 0.9875\n",
      "Epoch 274/300\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.0533 - acc: 0.9875\n",
      "Epoch 275/300\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.0518 - acc: 0.9875\n",
      "Epoch 276/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.0597 - acc: 0.9875\n",
      "Epoch 277/300\n",
      "80/80 [==============================] - 0s 92us/step - loss: 0.0545 - acc: 0.9875\n",
      "Epoch 278/300\n",
      "80/80 [==============================] - 0s 92us/step - loss: 0.0587 - acc: 0.9875\n",
      "Epoch 279/300\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.0597 - acc: 0.9750\n",
      "Epoch 280/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.0572 - acc: 0.9875\n",
      "Epoch 281/300\n",
      "80/80 [==============================] - 0s 115us/step - loss: 0.0680 - acc: 0.9750\n",
      "Epoch 282/300\n",
      "80/80 [==============================] - 0s 63us/step - loss: 0.0578 - acc: 0.9875\n",
      "Epoch 283/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.0543 - acc: 0.9750\n",
      "Epoch 284/300\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.0754 - acc: 0.9875\n",
      "Epoch 285/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.0584 - acc: 0.9875\n",
      "Epoch 286/300\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.0625 - acc: 0.9875\n",
      "Epoch 287/300\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.0583 - acc: 0.9875\n",
      "Epoch 288/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.0651 - acc: 0.9625\n",
      "Epoch 289/300\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0551 - acc: 0.9875\n",
      "Epoch 290/300\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.0598 - acc: 0.9875\n",
      "Epoch 291/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.0572 - acc: 0.9875\n",
      "Epoch 292/300\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.0552 - acc: 0.9875\n",
      "Epoch 293/300\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.0510 - acc: 0.9875\n",
      "Epoch 294/300\n",
      "80/80 [==============================] - 0s 115us/step - loss: 0.0679 - acc: 0.9625\n",
      "Epoch 295/300\n",
      "80/80 [==============================] - 0s 79us/step - loss: 0.0540 - acc: 0.9875\n",
      "Epoch 296/300\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.0569 - acc: 0.9750\n",
      "Epoch 297/300\n",
      "80/80 [==============================] - 0s 65us/step - loss: 0.0556 - acc: 0.9875\n",
      "Epoch 298/300\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.0541 - acc: 0.9875\n",
      "Epoch 299/300\n",
      "80/80 [==============================] - 0s 83us/step - loss: 0.0780 - acc: 0.9875\n",
      "Epoch 300/300\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.1069 - acc: 0.9625\n"
     ]
    }
   ],
   "source": [
    "model = SNN_Sequential(X=X_train,y=y_train,alpha=0.1,Iters=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想得到参数$W,b$可以使用```model.get_weights()```\n",
    "\n",
    "更多参数查看[weights](https://keras.io/getting-started/faq/#savingloading-only-a-models-weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.36868286,  1.569643  , -0.24451733, -1.2689612 ],\n",
       "        [-0.05042315, -1.7797827 , -0.964066  ,  2.5961614 ]],\n",
       "       dtype=float32),\n",
       " array([ 0.        , -0.72724706,  0.        ,  0.6370079 ], dtype=float32),\n",
       " array([[ 0.12699008],\n",
       "        [ 2.1070478 ],\n",
       "        [ 0.30486453],\n",
       "        [-2.4831476 ]], dtype=float32),\n",
       " array([-0.6088237], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = model.history.history['loss']\n",
    "acc = model.history.history['acc']\n",
    "plt.plot(loss,c='blue',label='loss')\n",
    "plt.plot(acc,c='orange',label='accuracy')\n",
    "plt.title('Loss and accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出对于SGD,我们只要迭代50次左右效果就已经很好了,所以一般SGD会比GD更加高效."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 predict and score\n",
    "\n",
    "(1) 使用```model.evaluate```预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_score(X,y,model):\n",
    "    loss,acc = model.evaluate(x=X,y=y)\n",
    "    return loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 3ms/step\n",
      "The test set loss: 0.014648223295807838, acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "loss,acc = predict_score(X=X_test,y=y_test,model=model)\n",
    "print('The test set loss: {}, acc: {}'.format(loss,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 71us/step\n",
      "The train set loss: 0.05135903768241405, acc: 0.9875\n"
     ]
    }
   ],
   "source": [
    "loss,acc = predict_score(X=X_train,y=y_train,model=model)\n",
    "print('The train set loss: {}, acc: {}'.format(loss,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 API Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 定义input-layers\n",
    "\n",
    "```python\n",
    "inputs = Input(shape=(n,))```\n",
    "\n",
    "同样只需要定义输入的特征,数据的形状为(m,n)\n",
    "\n",
    "(2) 定义hidden layer-1\n",
    "\n",
    "activation: Relu\n",
    "units: 4\n",
    "\n",
    "```python\n",
    "layer_1 = Dense(units=4,activation='relu')(inputs)```\n",
    "\n",
    "(3) 定义output-layer\n",
    "\n",
    "```python\n",
    "outputs = Dense(units=1,activation='sigmoid')(layer_1)```\n",
    "\n",
    "(3) 初始化model\n",
    "\n",
    "输入为我们定义的inputs,输出为最后一层(输出层,outputs)\n",
    "\n",
    "```python\n",
    "Model(inputs=inputs,outputs=layer_2)```\n",
    "\n",
    "(4) compile model\n",
    "\n",
    "(4.1) optimizer: SGD and learning rate\n",
    "\n",
    "(4.2) loss: binary cross entropy\n",
    "\n",
    "add metrics accuracy\n",
    "\n",
    "```python\n",
    "SGD = keras.optimizers.SGD(lr=alpha)\n",
    "loss = keras.losses.binary_crossentropy\n",
    "model.compile(optimizer=SGD,loss=loss,metrics=['accuracy'])```\n",
    "\n",
    "(5) fit model\n",
    "\n",
    "```python\n",
    "model.fit(...)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNN_API(X,y,alpha,Iters):\n",
    "    m,n = X.shape\n",
    "    inputs = Input(shape=(n,))\n",
    "    layer_1 = Dense(units=4,activation='relu')(inputs)\n",
    "    outputs = Dense(units=1,activation='sigmoid')(layer_1)\n",
    "    \n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    SGD = keras.optimizers.SGD(lr=alpha)\n",
    "    loss = keras.losses.binary_crossentropy\n",
    "    model.compile(optimizer=SGD,loss=loss,metrics=['accuracy'])\n",
    "    model.fit(x=X,y=y,epochs=Iters,verbose=0)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SNN_API(X=X_train,y=y_train,alpha=0.1,Iters=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 2ms/step\n",
      "The test set loss: 0.02019057236611843 acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "loss,acc = model.evaluate(x=X_test,y=y_test)\n",
    "print('The test set loss: {} acc: {}'.format(loss,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 58us/step\n",
      "The train set loss: 0.05726186111569405 acc: 0.9875\n"
     ]
    }
   ],
   "source": [
    "loss,acc = model.evaluate(x=X_train,y=y_train)\n",
    "print('The train set loss: {} acc: {}'.format(loss,acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
