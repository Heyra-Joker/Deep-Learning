{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShallowNeuralNetwork(Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Neural Network Representation 神经网络的表示\n",
    "\n",
    "首先我们来看看什么叫神经网络,如下图所示:\n",
    "\n",
    "<img src=\"../picture/04.png\" width=500 heigth=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 有x1,x2,x3竖向堆叠起来的向量,这就是输入层(Input layer)\n",
    "\n",
    "(2) 中间这一层我们称之为隐藏层(Hiden layer),在训练集中，隐藏层的节点的真正数值我们是无法知道的，所以称之为隐藏层，也就是我们只看得到输入的值和输出的值\n",
    "\n",
    "(3) 最后这一层只有一个节点，我们称之为输出层(Output Layer),它负责输出预测值$\\hat{y}$\n",
    "\n",
    "(4) 之前我们只用$X$代表输入值,现在可以用$a^{[0]}$来表示，a代表激活($activate$),它意味整网络中不同层的值,那么中间的一层我们用$a^{[1]}$表示,其中隐藏层$layer1$的第一个节点我们表示为$a_{1}^{[1]}$,第二个节点$a_{2}^{[1]}....a_{4}^{[1]}$,\n",
    "\n",
    "所以:\n",
    "\n",
    "$a^{[1]}=\\begin{bmatrix}\n",
    "a_1^{[1]}\\\\ a_1^{[2]}\n",
    "\\\\ a_1^{[3]}\n",
    "\\\\ a_1^{[4]}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "它是一个四维的,因为我们这里有4个节点或者说是隐藏单元.\n",
    "\n",
    "(5) 最后的输出层用$a^{[2]}$表示,只有一个节点,是一个实数,所以$\\hat{y} = a^{[2]}$\n",
    "\n",
    "其实就像Logistics regression中$\\hat{y}= a$,只是其只有一层.\n",
    "\n",
    "(6) 该图是两层的神经网络,因为我们一般不把第一层算入,也就是只有$hiden\\;layer\\;and\\;output\\;layer:2\\;layer\\;neural\\;network$\n",
    "\n",
    "(7) 最后我们要知道在隐藏层和最后的输出层是带有参数的:\n",
    "\n",
    "\n",
    "隐藏层有两个相关的参数$W^{[1]},b^{[1]}$,我们定义$W$是一个4x3的矩阵,$b$是一个4X1的向量,第一个数字4代表我们有4个隐藏单元或者节点,3代表我们有三个输入特征,也就说参数的第一个值为当前节点的个数,第二个值为输入的特征个数即:(units,inputs)形式的形状.\n",
    "\n",
    "于是按照这样的定义:\n",
    "\n",
    "输出层也有两个参数$W^{[2]},b^{[2]}$,分别是1X4,1X1的形式,这里的1X4是代表有输出层有1个节点,而输入层(也就是隐藏层有4个节点).\n",
    "\n",
    "[PDF](../PDFS/C1W3L02slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Comouting a Neural Network's Output 计算神经网络的输出(单个样本)\n",
    "\n",
    "![](../picture/05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上:\n",
    "\n",
    "左侧是最开始的Logistics regression的方式,先计算出z,然后通过sigmoid函数计算出预测值a.那么右侧的两层神经网络中的hiden layer就是重复上面的左侧的部分.\n",
    "\n",
    "也就是说:\n",
    "\n",
    "我们先取出右侧隐藏层中的第一个,那么经过左侧的运算将会得到\n",
    "\n",
    "$z^{[1]}_1 = w^{[1]^T}_{1} * X + b^{[1]}_1$这里的$[1]$代表第一层,下角标1代表第一个节点\n",
    "\n",
    "那么同样我们会有\n",
    "\n",
    "$z^{[1]}_2 = w^{[1]^T}_2 * X + b^{[1]}_2$\n",
    "\n",
    "$z^{[1]}_3 = w^{[1]^T}_3 * X + b^{[1]}_3$\n",
    "\n",
    "$z^{[1]}_4 = w^{[1]^T}_4 * X + b^{[1]}_4$\n",
    "\n",
    "我们经过向量化会得到:\n",
    "\n",
    "$z^{[1]}_{(4\\times 1)}=w^{[1]}_{(4\\times 3)}\\cdot X_{(3\\times 1)} + b^{[1]}_{(4\\times 1)}$\n",
    "\n",
    "通过Logistics regression我们可以知道如果我们创建的$w$是一个3X1的一个矩阵,因为输入的特征有3个,然后将其转置,那么就变为1X3的矩阵,经过堆叠,所以隐藏层的$W^{[1]}$是一个4X3的一个矩阵,也就是上一节中我们说到有4个神经元节点和3个输入特征.\n",
    "\n",
    "在经过计算那么$Z^{[1]}$的维度是4X1.\n",
    "\n",
    "然后我们可以将隐藏层看成是“输入层“,那么对于输出层而言:\n",
    "\n",
    "输入的特征是一个$a^{[1]} = active(Z^{[1]}_{(4\\times 1)})$的向量,那么:\n",
    "\n",
    "$z^{[2]}_{(1\\times 1)} = w^{[2]}_{(1\\times 4)} \\cdot a^{[1]}_{(4\\times 1)} + b_{(1\\times 1)}$\n",
    "\n",
    "所以$w^{[2]}$可以看成1个节点和4个输入\n",
    "\n",
    "最后,在二分类的情况下:\n",
    "\n",
    "$a^{[2]}_{(1\\times 1)}=sigmoid(z^{[2]}_{(1\\times 1)})$\n",
    "\n",
    "所以对于当前神经网络:\n",
    "\n",
    "$Z^{[1]}=W^{[1]}\\cdot X + b^{[1]}$\n",
    "\n",
    "$a^{[1]}=active(Z^{[1]})$\n",
    "\n",
    "$Z^{[2]}=W^{[2]}\\cdot a^{[1]} + b^{[2]}$\n",
    "\n",
    "$a^{[2]}=sigmoid(Z^{[2]})$\n",
    "\n",
    "以上的步骤我们称之为:**向前传播(Forward Propagation)**\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "注意无论$w,b$如何定义初始维度,只要最终的结果是正确的形状就行,因为这只是涉及到一个转置的问题.比如输入的样本是一个,那么输出的预测样本也应该是一个.只是我们采用(units,inputs)这种方式会比较好记忆."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Activation functions 激活函数\n",
    "\n",
    "细心的朋友会发现在hidden layer中是采用$active$的形式,这就是神经网中的激活函数:\n",
    "\n",
    "当你在搭建一个神经网络的时候,你可以选择在隐藏层或者输出层选择哪个激活函数去作用,目前我们一直使用的是sigmoid函数,但是事实证明在大多数隐藏层中使用其他的激活函数会比sigmoid更好.\n",
    "\n",
    "在一般情况下我们可以使用$g^{[1]}(z^{[1]})$来代表不同的激活函数,下面看看在神经网络中最基本也是最常用的激活函数:\n",
    "\n",
    "#### 3.1 tanh激活函数\n",
    "\n",
    "<img src=\"../picture/06.png\" width=500 heigth=\"500\">\n",
    "\n",
    "是用tanh的优缺点:\n",
    "\n",
    "(1) sigmoid 函数介于0-1之间,而对于隐藏层而言,thanh(双曲正切，实际上就是sigmoid平移后)函数,其值介于[-1,1]之间,效果会比sigmoid函数好一些,因为输出的值是介于[-1,1]之间,那么激活函数的均值就更加接近于0,使得数据更加中心化,让下一层神经网络更加容易学习.\n",
    "\n",
    "(2) 基本现在在隐藏层都不会使用sigmoid激活函数,thanh在更多情况下的效果都会更好,但是对于输出层而言,$y =1 | y = 0$,是一个二分类问题,那么可以继续使用sigmoid函数使得$\\hat{y}$介于[0,1]之间.\n",
    "\n",
    "(3) tanh和sigmoid函数都有一个缺点,那就是在$Z$非常大或者非常小的时候,那么导数的梯度就会变得非常的小,接近于0,这样会拖慢梯度下降法.\n",
    "\n",
    "(4) 其导数为:\n",
    "\n",
    " $dz = 1 - (thanh(z))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 ReLu(rectified linear uint)激活函数\n",
    "\n",
    "<img src=\"../picture/07.png\" width=500 heigth=\"500\">\n",
    "\n",
    "(1) 基本现在隐藏层中都是一个默认的选项,如果你不确定隐藏层中应该使用哪个,那么可以尝试先使用Relu.\n",
    "\n",
    "(2) 只要$Z$为正,导数就是1,$Z$为负导数就为0.另外如果$Z=0.000000000$,那么导数没有定义,实际中遇见$Z=0$的机率非常低,如果真的遇到了可以给导数赋值比如0,1都是可以的.\n",
    "\n",
    "(3) 是用ReLu激活函数的好处是,在多个隐藏层中使用ReLu的话会使得在大部分空间$Z$上,导数离0远,这样在使用GD的时候会产生更大的梯度,这会使得你的神经网络学习速度会快很多,因为其不像sigmoid和tanh函数斜率接近于0.\n",
    "\n",
    "(4) 其导数为:$dz=\\left\\{\\begin{matrix}\n",
    "0 &if\\; z<0 \\\\ \n",
    " 1&if \\; z>0 \n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "如果觉得ReLu激活函数在负数部分太过于强势,那么可以选择:\n",
    "\n",
    "**带泄漏的ReLu(leaky ReLu)**\n",
    "\n",
    "<img src=\"../picture/08.png\" width=500 heigth=\"500\">\n",
    "\n",
    "但是在实际过程中使用率并不高,有的人说当$Z$为负数的时候,滞漏0.01的效果会很好,但是具体情况还是需要具体分析.\n",
    "\n",
    "其导数为:$dz=\\left\\{\\begin{matrix}\n",
    "0.01 &if\\; z<0 \\\\ \n",
    " 1&if \\; z>0 \n",
    "\\end{matrix}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后:\n",
    "\n",
    "除非是在做二元分类的输出层,否则尽可能的不去使用sigmoid激活函数.\n",
    "\n",
    "通常在隐藏层我们会使用thanh激活函数或者ReLu,如果愿意你也可以尝试一下leaky ReLu.\n",
    "\n",
    "[常用激活函数](https://keras.io/zh/activations/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Why do you need non-linear activation function\n",
    "\n",
    "为什么我们要是用非线性激活函数呢?\n",
    "\n",
    "对于:\n",
    "\n",
    "$Given X:$\n",
    "\n",
    "$Z^{[1]}=W^{[1]}\\cdot X + b^{[1]}$\n",
    "\n",
    "$a^{[1]}=g^{[1]}(Z^{[1]})$\n",
    "\n",
    "$Z^{[2]}=W^{[2]}\\cdot a^{[1]} + b^{[2]}$\n",
    "\n",
    "$a^{[2]}=g^{[2]}(Z^{[2]})$\n",
    "\n",
    "我们去掉$ a^{[1]},a^{[2]}$也就是 \"linear activation function\"(恒等函数)\n",
    "\n",
    "(1) 那么这个模型输出的$\\hat{y}$,不过就是输入特征x的线性组合,也就是说无论有多少层隐藏层,都只是一直在计算线性激活函数,所以不如直接去掉隐藏层.即隐藏层没有任何作用,根据经典机器学习中LR的知识,线性函数是无法做分类的,也很容易受到奇点的影响.\n",
    "\n",
    "(2) 如果我们在隐藏层中使用linear function,在输出层使用sigmoid function,那么实际上其就是一个Logistics regression(因为layer1输出的就是一个线性函数,在作用sigmoid).那么在上一节我们也看到了,LR对于图片二分类来说还是有点力不足的情况.\n",
    "\n",
    "(3) 如果你做的是回归的问题而不是二元分类,那么在隐藏层中使用linear function也许可能有用,因为输出的值将是一个实数.\n",
    "\n",
    "(4) 所以在隐藏层可以使用thanh或者ReLu或者leaky ReLu,现在一般对于二元分类我们只会在输出层使用sigmoid函数.\n",
    "\n",
    "\n",
    "[PDF](../PDFS/C1W3L07slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Gradient descent for neural networks 神经网络的梯度下降法\n",
    "\n",
    "神经网络中的梯度下降法:\n",
    "\n",
    "![](../picture/09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实神经网络的GD和LR中的GD是一样的,只不过是多了几步,同样使用链式法则:\n",
    "\n",
    "对于现在如下情况:\n",
    "\n",
    "**Forward Propagation:**\n",
    "\n",
    "$Z^{[1]}=W^{[1]}\\cdot X + b^{[1]}$\n",
    "\n",
    "$a^{[1]}=g^{[1]}(Z^{[1]})$\n",
    "\n",
    "$Z^{[2]}=W^{[2]}\\cdot a^{[1]} + b^{[2]}$\n",
    "\n",
    "$a^{[2]}=sigmoid(Z^{[2]})$\n",
    "\n",
    "**Backward Propagation(Single example):**\n",
    "\n",
    "\n",
    "从输出层到最后一个隐藏层实际上就是LR的链式法则:\n",
    "\n",
    "对于loss函数:\n",
    "\n",
    "$\\frac{\\partial{L(a^{[2]},y)}}{\\partial a^{[2]}} = -y\\cdot log(a^{[2]}) - (1-y)\\cdot log(1-a^{[2]})$\n",
    "\n",
    "$da^{[2]} = \\frac{\\partial{L(a^{[2]},y)}}{\\partial{a^{[2]}}} = \\frac{1-y}{1-a^{[2]}} - \\frac{ y}{ a^{[2]}}$\n",
    "\n",
    "$dz^{[2]} = \\frac{\\partial{L(a^{[2]},y)}}{\\partial{a^{[2]}}}\\cdot \\frac{\\partial{a^{[2]}}}{\\partial{z^{[2]}}}=da^{[2]}\\cdot sigmoid(z^{[2]})^{'}=da^{[2]}\\cdot a^{[2]}(1-a^{[2]})=a^{[2]}-y$\n",
    "\n",
    "$dw^{[2]} = \\frac{\\partial{L(a^{[2]},y)}}{\\partial{a^{[2]}}}\\cdot \\frac{\\partial{a^{[2]}}}{\\partial{z^{[2]}}}\\cdot \n",
    "\\frac{\\partial{z^{[2]}}}{\\partial{w^{[2]}}}=dz^{[2]}\\cdot a^{[1]}$\n",
    "\n",
    "$db^{[2]} = \\frac{\\partial{L(a^{[2]},y)}}{\\partial{a^{[2]}}}\\cdot \\frac{\\partial{a^{[2]}}}{\\partial{z^{[2]}}}\\cdot \n",
    "\\frac{\\partial{z^{[2]}}}{\\partial{b^{[2]}}}=dz^{[2]}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$da^{[1]}=\\frac{\\partial{L(a^{[2]},y)}}{\\partial{a^{[2]}}}\\cdot \\frac{\\partial{a^{[2]}}}{\\partial{z^{[2]}}}\\cdot \n",
    "\\frac{\\partial{z^{[2]}}}{\\partial{a^{[1]}}}=w^{[2]}\\cdot dz^{[2]}$\n",
    "\n",
    "$dz^{[1]}=\\frac{\\partial{L(a^{[2]},y)}}{\\partial{a^{[2]}}}\\cdot \\frac{\\partial{a^{[2]}}}{\\partial{z^{[2]}}}\\cdot \n",
    "\\frac{\\partial{z^{[2]}}}{\\partial{a^{[1]}}}\\cdot \\frac{\\partial{a^{[1]}}}{\\partial{z^{[1]}}}=w^{[2]}\\cdot dz^{[2]} \\cdot g^{[1]^{'}}(z^{[1]})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$dw^{[1]}=\\frac{\\partial{L(a^{[2]},y)}}{\\partial{a^{[2]}}}\\cdot \\frac{\\partial{a^{[2]}}}{\\partial{z^{[2]}}}\\cdot \n",
    "\\frac{\\partial{z^{[2]}}}{\\partial{a^{[1]}}}\\cdot \\frac{\\partial{a^{[1]}}}{\\partial{z^{[1]}}}\\cdot \\frac{\\partial{z^{[1]}}}{\\partial{w^{[1]}}}=dz^{[1]}\\cdot X$\n",
    "\n",
    "$db^{[1]}=\\frac{\\partial{L(a^{[2]},y)}}{\\partial{a^{[2]}}}\\cdot \\frac{\\partial{a^{[2]}}}{\\partial{z^{[2]}}}\\cdot \n",
    "\\frac{\\partial{z^{[2]}}}{\\partial{a^{[1]}}}\\cdot \\frac{\\partial{a^{[1]}}}{\\partial{z^{[1]}}}\\cdot \\frac{\\partial{z^{[1]}}}{\\partial{b^{[1]}}}=dz^{[1]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中如果激活函数$g^{[1]}$:\n",
    "\n",
    "ReLu:$g^{[1]^{'}}=\\left\\{\\begin{matrix}\n",
    "0 &if\\; z<0 \\\\ \n",
    " 1&if \\; z>0 \n",
    "\\end{matrix}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tanh:$g^{[1]^{'}} = 1 - (thanh(z))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么对于M example-vectorize(Python):\n",
    "\n",
    "$dZ^{[2]} = A^{[2]} - Y\\;shape(n^{[2]},m)\\;Y = [y^{(1)},y^{(2)},...,y^{(m)}]$\n",
    "\n",
    "$dW^{[2]} = \\frac{1}{m} *  dz^{[2]} A^{[1]^{T} }\\;shape(n^{[2]},n^{[1]})$ shape equal $w^{[2]}$\n",
    "\n",
    "$db^{[2]} = \\frac{1}{m} * np.sum(dZ^{[2]}, axis = 1,keepdims = True) $ shape equal $b^{[2]}$\n",
    "\n",
    "$dZ^{[1]} = W^{[2]^{T}}dZ^{[2]}\"*\"( g^{[1]^{'}}(Z^{[1]}))$ \n",
    "\n",
    "$dW^{[1]} = \\frac{1}{m}*dZ^{[1]}X^{T}  shape(n^{[1]},n^{[0]})$ shape equal $W^{[1]}$\n",
    "\n",
    "$db^{[1]} = \\frac{1}{m}*np.sum(dz^{[1]},axis = 1,keepdims = True)$ shape equal $b^{[1]}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "\n",
    "其中\"*\"表示逐个元素的乘积,使用$\\frac{1}{m}$是为了平均m个样本.\n",
    "\n",
    "在Numpy中矩阵相乘我们使用的是$np.dot()$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里我们就已经知道了神经网络的FP(Forward Propagation)和BP(Backward Propagation),实际上神经网络就是一个FP和BP的过程.最后再进行参数更新\n",
    "\n",
    "$W^{[2]} = W^{[2]} - \\alpha\\cdot dW^{[2]}$\n",
    "\n",
    "$b^{[2]} = b^{[2]} - \\alpha\\cdot db^{[2]}$\n",
    "\n",
    "$W^{[1]} = W^{[1]} - \\alpha\\cdot dW^{[1]}$\n",
    "\n",
    "$b^{[1]} = b^{[1]} - \\alpha\\cdot db^{[1]}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Random Initialization 随机初始化\n",
    "\n",
    "对于Logistics Regression 可以将权重初始化为0，但是如果对神经网络也同样初始化为0，那么再使用梯度下降法那么会完全无效.\n",
    "\n",
    "<img src=\"../picture/10.png\" width=500 heigth=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果零$w$初始化全为0,那么$w^{[1]}_{1} = w^{[1]}_{2}$,所以$a^{[1]}_{1}和a^{[1]}_{2}$计算的是同一个函数,那么隐藏单元或者节点的计算就是毫无意义的.所以,无论隐藏层有多少个节点,计算的都是相同的函数,那么也是毫无意义的.故不能将其全部初始化为0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应该使:\n",
    "\n",
    "$w^{[1]} = np.random.randn((2,2)) * 0.01$ 0.01可以乘也可以不乘\n",
    "\n",
    "$b^{[1]} = np.zeros((2,1))$ 可以初始化为0,因为只要$w$不同,那么隐藏单元计算的就是不同的函数\n",
    "\n",
    "$w^{[2]} = np.random.randn((2,2)) * 0.01$\n",
    "\n",
    "$ b^{[2]} = np.zeros((2,1)) $\n",
    "\n",
    "我们一般要把权重矩阵初始化一个基本小的值,如果权重值给的太大或者太小,那么在输出层sigmoid函数最平缓的部分,进而使用梯度下降法的速度就会变的很慢,但是如果你最后的输出层不是使用sigmoid激活函数,那么问题可能并不那么大.\n",
    "\n",
    "在浅层神经网络的时候,乘上0.01并没有多少问题,但是在深层神经网络的时候,那么应该试一下其他参数.到后面会继续详诉.无论怎样,权重应该是一个不要太大也不要太小的值."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Summary\n",
    "\n",
    "我们使用了浅层神经网络来说明整个神经网络的流程,实际对于现实开发而言,大多数会使用深层神经网络.另外对于神经网络的科学解释也正在探索过程中,可能某一天会得到一个科学的解释.\n",
    "\n",
    "神经网络的基本步骤:\n",
    "\n",
    "(1) 初始化参数\n",
    "\n",
    "(2) 向前传播\n",
    "\n",
    "(3) 向后传播\n",
    "\n",
    "(4) 更新参数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
