{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepNeuralNetwork(Theory)\n",
    "\n",
    "### 1 DNN\n",
    "\n",
    "![](../picture/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前我们已经知道单层的神经网络(Logistics regression)还有shallow neural network,其实深度神经网络只是一个深度的问题,根据以往的经验,很多激活函数只适合深度神经网络且一些浅层神经网络是无法学习的,虽然一个项目之前要寻找到最适合的神经层数是非常难的,但是可以先试试看最基本的Logistics regression是非常合理的做法,然后在一层一层的添加,最后使用交叉验证来检测你的准确度."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个4层的神经网络,也就是3层隐藏层,其中每层中有5,5,3个隐藏神经节点,最后一个输出层:\n",
    "\n",
    "符号定义:\n",
    "\n",
    "L 代表神经网络的层数:L = 4 (layers)\n",
    "\n",
    "$n^{[l]} = units\\;in\\;layer$\n",
    "\n",
    "代表每层隐藏层下面的节点数或者神经单元数量,比如$n^{[1]} = 5 = n^{[2]},n^{[4]} = n^{[L]} = 1,n^{[0]} = n_x = 3$\n",
    "\n",
    "$a^{[l]} = activations\\;in\\;layer\\;l$ 层的激活函数为$a^{[l]}$,$a^{[0]} = x,\\hat{y} = a^{[L]}$\n",
    "\n",
    "$a^{[l]} = g^{[l]}(z^{[l]})$\n",
    "\n",
    "$w^{[l]} = weigths\\;in\\;layer$ $l$ 层的权重矩阵\n",
    "\n",
    "$b^{[l]} = bias\\;in\\;layer$   $l$ 层的偏置矩阵\n",
    "\n",
    "\n",
    "[PDF](../PDFS/C1W4L01DeepLLayerNN_annotated.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Forward Propagation in a Deep Network\n",
    "\n",
    "**Single example:**\n",
    "\n",
    "$z^{[1]} = w^{[1]}x + b^{[1]}$\n",
    "\n",
    "$a^{[1]} = g^{[1]}(z^{[1]})$\n",
    "\n",
    "$z^{[2]} = w^{[2]}a^{[1]} + b^{[2]}$\n",
    "\n",
    "$a^{[2]} = g^{[2]}(z^{[2]})$\n",
    "\n",
    "$z^{[3]} = w^{[3]} a^{[2]} + b^{[3]}$\n",
    "\n",
    "$a^{[3]} = g^{[3]}(z^{[3]})$\n",
    "\n",
    "$z^{[4]} = w^{[4]} a^{[3]}+ b^{[4]}$\n",
    "\n",
    "$a^{[4]} = g^{[4]}(z^{[4]}) = \\hat{y}$\n",
    "\n",
    "整体形式类为:\n",
    "\n",
    "$z^{[l]} = w^{[l]} a^{[l-1]} + b^{[l]}$\n",
    "\n",
    "$a^{[l]} = g^{[l]}(z^{[l]})$\n",
    "\n",
    "可以看出DNN与SNN没有多大区别,只是加大深度.\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "$g^{[l]}:$为activate function or optimizer\n",
    "\n",
    "如果做的是二分类那么$g^{[L]}=sigmoid$\n",
    "\n",
    "具体代码形式将在DeepNeuralNetwork中体现\n",
    "\n",
    "\n",
    "[PDF](../PDFS/C1W4L02ForwardPropInDN_annotated.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Backward Propagation in a Deep Network\n",
    "\n",
    "根据链式法则一样可以求出各个hidden layer的梯度.\n",
    "\n",
    "**Single example:**\n",
    "\n",
    "$da^{[4]} = \\frac{\\partial{L}}{\\partial{a^{[4]}}} = \\frac{1-y}{1-a^{[4]}}- \\frac{y}{a^{[4]}}$\n",
    "\n",
    "$dz^{[4]} = \\frac{\\partial{L}}{\\partial{a^{[4]}}}\\cdot \\frac{\\partial{a}^{[4]}}{\\partial{z}^{[4]}}=d{a^{[4]}}g^{[4]^{'}}(z^{[4]})=(\\frac{1-y}{1-a^{[4]}}-\\frac{y}{a^{[4]}})\\cdot a^{[4]}(1-a^{[4]})=a^{[4]}-y$\n",
    "\n",
    "$dw^{[4]} = \\frac{\\partial{L}}{da^{[4]}}\\cdot \\frac{\\partial{a^{[4]}}}{\\partial{z^{[4]}}}\\cdot\\frac{\\partial{z^{[4]}}}{\\partial{w^{[4]}}}= dz^{[4]} a^{[3]}$\n",
    "\n",
    "$db^{[4]} = \\frac{\\partial{L}}{\\partial{a^{[4]}}}\\cdot\\frac{\\partial{a^{[4]}}}{dz^{[4]}}\\cdot\\frac{\\partial{z^{[4]}}}{\\partial{b^{[4]}}}=dz^{[4]}$\n",
    "\n",
    "$da^{[3]} = \\frac{\\partial{L}}{\\partial{a^{[4]}}}\\cdot \\frac{\\partial{a^{[4]}}}{\\partial{z^{[4]}}}\\cdot \\frac{dz^{[4]}} {da^{[3]}} = w^{[4]}\\cdot dz^{[4]}$\n",
    "\n",
    "$dz^{[3]} = \\frac{\\partial{L}}{\\partial{a^{[4]}}}\\cdot\\frac{ \\partial{a^{[4]}}}{\\partial{z^{[4]}}}\\cdot\\frac{\\partial{z^{[4]}}}{\\partial{a^{[3]}}}\\cdot \\frac{\\partial{a^{[3]}}}{\\partial{z^{[3]}}} = da^{[3]} * g^{[3]^{'}}(z^{[3]})$\n",
    "\n",
    "$dw^{[3]} = \\frac{\\partial{L}}{\\partial{a^{[4]}}}\\cdot \\frac{\\partial{a^{[4]}}}{\\partial{z^{[4]}}}\\cdot \\frac{\\partial{z^{[4]}}}{\\partial{a^{[3]}}}\\cdot\\frac{\\partial{a^{[3]}}}{\\partial{z^{[3]}}}\\cdot\\frac{\\partial{z^{[3]}}}{\\partial{w^{[3]}}} = dz^{[3]} a^{[2]}$\n",
    "\n",
    "$db^{[3]} = \\frac{\\partial{L}}{\\partial{a^{[4]}}}\\cdot \\frac{\\partial{a^{[4]}}}{\\partial{z^{[4]}}}\\cdot \\frac{\\partial{z^{[4]}}}{\\partial{a^{[3]}}}\\cdot\\frac{\\partial{a^{[3]}}}{\\partial{z^{[3]}}}\\cdot\\frac{\\partial{z^{[3]}}}{\\partial{b^{[3]}}} = dz^{[3]}$\n",
    "\n",
    "$\\cdots$\n",
    "\n",
    "$da^{[1]} = \\frac{\\partial{L}}{\\partial{a^{[1]}}}= \\cdots = w^{[2]} dz^{[2]}$\n",
    "\n",
    "$dz^{[1]} =  \\frac{\\partial{L}}{\\partial{z^{[1]}}} = \\cdots = da^{[1]} * g^{[1]^{'}}(z^{[1]})$\n",
    "\n",
    "$dw^{[1]} = dz^{[1]} a^{[0]}$\n",
    "\n",
    "$db^{[1]} = dz^{[1]}$\n",
    "\n",
    "总结为(m-example):\n",
    "\n",
    "$dZ^{[l]} = dA^{[l]} * g^{[l]^{'}}(Z^{[l]})$\n",
    "\n",
    "$dW^{[l]} = \\frac{1}{m} dZ^{[l]} A^{[l-1]}$\n",
    "\n",
    "$db^{[l]} =\\frac{1}{m} np.sum(dZ^{[l]} ,axis = 1, keepdim = True)$\n",
    "\n",
    "$dA^{[l-1]} = W^{[l]^{T}} dZ^{[l]}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样反向传播中:\n",
    "\n",
    "$shape\\;dA^{[l]} = A^{[l]}$\n",
    "\n",
    "$shape\\;dZ^{[l]} = Z^{[l]}$\n",
    "\n",
    "$shape\\;dW^{[l]} = W^{[l]}$\n",
    "\n",
    "$shape\\;db^{[l]} = b^{[l]}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无论inputs values如何定义形状,那么还是那句话只是涉及一个转置的问题\n",
    "\n",
    "[PDF](../PDFS/C1W4L06slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Parameters VS Hyperparameters\n",
    "\n",
    "想要你的深度神经网络起很好的效果,你要规划好你的参数以及超级参数\n",
    "\n",
    "Parameters: \n",
    "\n",
    "$W^{[1]},b^{[1]},W^{[2]},b^{[2]}.....$\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "(1) Learning rate alpha\n",
    "\n",
    "(2) iterations\n",
    "\n",
    "(3) hiden layer L\n",
    "\n",
    "(4) hiden unites\n",
    "\n",
    "(5) choice of activation function\n",
    "\n",
    "(6) choice of optimizer function \n",
    "\n",
    "实际上在选择超级参数的时候大多数还是需要依靠经验(当然也有系统的方法后面会说到),比如先选定一个alpha进行测试,看看损失函数$loss$有没有下降,如果更换了alpha,损失函数如果上升了,那么就可以缩小alpha的范围.\n",
    "\n",
    "另外如果你已经使用了一个很久的模型,在某些情况下是需要改变最优学习率的,因为随着数据的不同(或者电脑老化比如cpu,GPU)学习率也会逐渐变为不是最优 ,所以我们基本会隔一段时间再去检验有没有更好的超级参数数值.\n",
    "\n",
    "[PDF](../PDFS/C1W4L07notes.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
