{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RegularizationAndDropout(Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Train/dev/test sets\n",
    "\n",
    "实际上在训练模型的时候,对于数据集的划分我们有一下几种情况:\n",
    "\n",
    "假设现在我有一个Data set,其拆分为training set(训练集),development set(\"dev set\",交叉验证集),test set(测试集)\n",
    "\n",
    "(1) 在机器学习小数据量的时代通常将数据7/3分,也就是70%的训练集和30%的测试集,或者6/2/2分,60% training set/20% dev set/ 20% test set\n",
    "\n",
    "(2) 在大数据时代(百万级别):通常分为98/1/1,98% training set/ 1% dev set/ 1% test set,对于更高的可能分为99.5/0.25/0.25\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "验证集就是检测验证那种算法更加有效\n",
    "\n",
    "测试集的主要目的是正确评估分类器的性能\n",
    "\n",
    "通过训练集训练数据,通过交叉验证机选择最好的模型,通过测试集去进行无偏评估算法的运行状况.\n",
    "\n",
    "\n",
    "另外:现代深度学习的另外一个趋势是越来越多的人采用在训练集和测试集分布不匹配的情况下进行训练\n",
    "\n",
    "Training set:\n",
    "  - Cat pictures from webpages\n",
    "  - 经过后期制作精良\n",
    "Dev/ test sets:\n",
    "  - Cat pictures from users \n",
    "  - 摄像粗糙、像素低\n",
    "\n",
    "对于这种情况,要确保验证集和测试集的数据来自同一分布(Make sure dev set test set come from dame disturbution),因为你要用验证集来评估不同的模型\n",
    "\n",
    "对于数据采集可以使用网页抓取,但是代价就是训练集数据与验证集数据和测试集数据有可能不是来自同一分布，在后面我们会更加详细的解释这条经验法则.\n",
    "\n",
    "\n",
    "[PDF](../../PDFS/C2W1L01notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Bias/ Variance\n",
    "\n",
    "在训练结果中我们经常会遇到两种情况:\n",
    "\n",
    "(1) 过拟合(over fitting): 太注重训练样本,从而使得测试样本的正确率会很低训练样本的正确率高.也就是说太注重一棵树而放弃整个森林.\n",
    "\n",
    "(2) 欠拟合(under fitting): 太过于随意,从而使得测试样本和训练样本的正确率会忽高忽低.也就是说太浪了.\n",
    "\n",
    "![](../../picture/17.png)\n",
    "\n",
    "对于第一种属于高偏差:undefitting(欠拟合)\n",
    "\n",
    "第二种属于正常\n",
    "\n",
    "第三种属于高方差:overfitting(过拟合)\n",
    "\n",
    "比如说我们对于一个cat Classification:\n",
    "\n",
    "<img src=\"../../picture/18.png\" width=300 height=300>\n",
    "\n",
    "假设肉眼误差(human Error): $\\cong 0\\%$\n",
    "\n",
    "那么:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>Train set Error</td>\n",
    "        <td>1%</td>\n",
    "        <td>15%</td>\n",
    "        <td>15%</td>\n",
    "        <td>0.5%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Dev/Test set Error</td>\n",
    "        <td>11%</td>\n",
    "        <td>16%</td>\n",
    "        <td>30%</td>\n",
    "        <td>1%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>result</td>\n",
    "        <td>high variance</td>\n",
    "        <td>high bias</td>\n",
    "        <td>high v.b</td>\n",
    "        <td>OK</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是说:\n",
    "\n",
    "对于第一种情况是过拟合,训练样本错误率低,而测试(验证)样本错误率高.\n",
    "\n",
    "对于第二,三种情况都是属于欠拟合,训练样本和测试样本的错误率都高.\n",
    "\n",
    "对于第三种情况是最好的一种情况.\n",
    "\n",
    "从整体的训练/验证集上来看这几种的效果如下(reference Stanford):\n",
    "\n",
    "<img src=\"../../picture/68.png\" width=500 height=500>\n",
    "\n",
    "[PDF](../../PDFS/C2W1L02notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Basic \"recipe\" for machine learning\n",
    "\n",
    "为了解决上诉的问题,我们这里提供一些基本的选择方法\n",
    "\n",
    "如果是high bias:\n",
    "\n",
    "(1) 那么我们尝试可以建立更深的网络(神经节点或者隐藏层数).\n",
    "\n",
    "(2) 增大训练次数.\n",
    "\n",
    "(3) 更换optimizer或者activate function.\n",
    "\n",
    "如果是high variance:\n",
    "\n",
    "(1) 喂入更多的数据,通常这个办法是比较好的\n",
    "\n",
    "(2) 使用regularization(正则化),Dropout来减少过拟合.\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "通常会使用训练集来验证诊断算法是否存在偏差或者方差的问题.\n",
    "\n",
    "在机器学习的初期阶段,关于所谓的偏差方差权衡探讨屡见不鲜\"Bias Variance tradeoff\",我们有很多方法可以使得两者呈现反向关系,也就是bias增大,variance减少,很难有方法可以只减少偏差或者方差而不对另一方有影响,但是在现代深度学习,大数据时代,只要有更大的神经网络以及更多的数据,在正则化较好的情况下,是可以实现在不过多影响对方的前提下减少自己,但是这并不意味着神经网络越深越好,因为我们的目标还是要服从\n",
    "\n",
    "奥卡姆剃刀(Occam's razor)原理:在所有可能选择的模型中,我们应该选择能够很好地解释已知数据并且十分简单的模型.也就是说只要简单的神经网络模型能够将问题很好的解决,我们就没有必要使用复杂的模型.\n",
    "\n",
    "[PDF](../../PDFS/C2W1L03pdf.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2  Regulariztion\n",
    "如果你怀疑模型过拟合了,那么可以尝试使用正则化,这里我们只是简单阐述一下正则化(L1,L2 范数),更多关于正则化的理论请查看Classic_machine_learning库中的[4-2 Logistic Regression(Theory02)](https://github.com/woaij100/Classic_machine_learning/blob/master/4-2%20Logistic%20Regression(Theory02).ipynb)\n",
    "\n",
    "正则化的目的是将参数稀疏,举个简单的例子,如果我们要判断某张图片是否是猫,那么对于特征而言,只有构成猫的轮廓的像素点特征是重要的,其余的比如天空,小草等背景是次要的,正则化能够将某些不重要的特征处理为0或者接近于0.这样就使得我们能够保存相对于重要的特征.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们的损失函数的形式如下:\n",
    "\n",
    "$J(w,b) = \\frac{1}{m}\\cdot \\sum_{i=1}^{m}(L( \\hat{y}_{i},y_{i}))\\;w\\in R^{n_x},b\\in R$\n",
    "\n",
    "**L1 regularization:**\n",
    "\n",
    "$||x||_1 :=\\sum_{i=1}^{n}|x_i|^{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么损失函数加入L1正则化以后的形式为:\n",
    "\n",
    "$J(w,b) = \\frac{1}{m}\\cdot \\sum_{i=1}^{m}(L( \\hat{y}_{i},y_{i})) + \\frac{\\lambda}{m} \\sum_{i=1}^{n}|w_i|^{1}_1$\n",
    "\n",
    "其中$\\lambda$为超级参数,需要自己调控来控制稀疏程度,等于0的时候代表不稀疏化.\n",
    "\n",
    "这里除上$m$是为了标量化一下."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其参数更新的时候会有以下形式:\n",
    "\n",
    "$w := w-\\alpha(dw+\\frac{\\lambda}{m} sgn(w_i))=w-\\alpha \\frac{\\lambda}{m} sgn(w_i) -\\alpha dw$ \n",
    "\n",
    "$sgn(w_i)>0:1$\n",
    "\n",
    "$sgn(w_i)<0:-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以参数更新的时候能够实现权重$W$稀疏.另外对于bias也可以使用正则化,但是由于bias只是一个常数项,正则化意义也不是很大,当然加上也没有多少问题.\n",
    "\n",
    "如果加上损失函数会有如下形式:\n",
    "\n",
    "$J(w,b) = \\frac{1}{m}\\cdot \\sum_{i=1}^{m}(L( \\hat{y}_{i},y_{i})) + \\lambda \\sum_{i=1}^{n}|w_i|^{1}_1 + \\lambda |b_i|^{1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L2 regularization:**\n",
    "\n",
    "$||x||_2 :=(\\sum_{i=1}^{n}|x_i|^{2})^{\\frac{1}{2}}$\n",
    "\n",
    "那么损失函数加入L2正则化以后的形式为:\n",
    "\n",
    "这里我们为了处理方便将$L_2$范数取平方(平方并不更改几何形状)即:$||w_i||^{2}_{2}=(||w_i||_{2}^\\frac{1}{2})^2$\n",
    "\n",
    "$J(w,b) = \\frac{1}{m}\\cdot \\sum_{i=1}^{m}(L( \\hat{y}_{i},y_{i})) + \\frac{\\lambda}{2m} \\sum_{i=1}^{n}|w_i|^{2}_2$\n",
    "\n",
    "其中$\\lambda$为超级参数,需要自己调控来控制稀疏程度,等于0的时候代表不稀疏化.\n",
    "\n",
    "这里除上$2m$是为了求导数时能够将2给消除,$m$是做一下标量化."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其参数更新的时候会有以下形式:\n",
    "\n",
    "$w := w-\\alpha(dw+\\frac{\\lambda}{m} \\cdot w_i)=w-\\alpha \\frac{\\lambda}{m} \\cdot w_i -\\alpha dw$\n",
    "\n",
    "所以参数更新的时候也会实现$W$稀疏,但是相对于$L1$而言,稀疏的不那么强硬,俗话说再小的东西也有重要的时候,所以我们也需要保留一些不重要的东西."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:orange\">实际上$L_2$范数会比$L_1$范数更加实用,因为$L_2$只是做衰减(decay),只会将$w_i$尽量向0靠近,减少权重,而$L_1$比较猛,直接就是向0使劲靠.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**图像化解释$L_2$优于$L_1$** \n",
    "\n",
    "<img src=\"../../picture/19.png\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在图中,左边是$L_1$,右边是$L_2$,可以看出:\n",
    "\n",
    "对于$L_1$因为有菱角的时候,所以只有目标函数摆的非常好的时候.参数稀疏的可能才会大(也就是两个图的交点)\n",
    "\n",
    "\n",
    "而$L_2$没有菱角性质,只要有相交的地方即可稀疏参数,所以$L_2$比$L_1$更加实用."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "\n",
    "在多维的情况下:\n",
    "\n",
    "由于L2范数是属于欧几里得空间向量,Frobenius范数表示所有大小矩阵的向量空间(shape:m,n)\n",
    "\n",
    "所以我们在神经网络将L2称之为Frobenius.\n",
    "\n",
    "$||A||_{F}=(\\sum_{i=1}^{m}\\sum_{j=1}^{n}|a_{ij}|^{2})^{\\frac{1}{2}}$\n",
    "\n",
    "该范数称作Frobenius(弗罗贝尼乌斯范数,矩阵范数 $||\\cdot||_F$),所以L2损失可以改写为:\n",
    "\n",
    "$J(w,b) = \\frac{1}{m}\\cdot \\sum_{i=1}^{m}(L( \\hat{y}_{i},y_{i})) + \\lambda \\sum_{i=1}^{n}|w_i^{2}|_F$\n",
    "\n",
    "\n",
    "[PDF](../../PDFS/C2W1L04notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Why regularizationn reduces overfitting\n",
    "\n",
    "我们在神经网络的角度来看看正则化为什么可以减少过拟合.\n",
    "\n",
    "我们已经知道正则化实际上是将$W$变得稀疏那么就相当于在网络结构中**近似于消除某些神经元节点**\n",
    "\n",
    "<img src=\"../../picture/20.png\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到,当使用正则化后,某些权重$W$会近似于0(当然不可能完全为0).那么就相当于消除某些神经元,使得过程变成类似于一个简单的LR.\n",
    "\n",
    "<img src=\"../../picture/21.png\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是说如果神经网络是过拟合的,那么将会朝向欠拟合的方向进行,那么应该会存在一个\"just right\"的$\\lambda$使得网络正好的处在既不过拟合也不欠拟合的状态."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外我们也可以从activate function的角度去看,这里使用的是tanh函数:\n",
    "\n",
    "<img src=\"../../picture/22.png\" width=500 heigth=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当使用正则化后,$W^{[l]}$朝着0的方向走,那么$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$中的$Z^{[l]}$也会朝着0的方向走,可以理解为几乎每一层都是线性的(红色部分),以前我们说过,在神经线中如果所有层都是在做线性操作,那么只是在重复而已,所以从这个角度看,正则化也是在减少过拟合."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Dropout\n",
    "\n",
    "除了L2正则化,还有一个非常实用的正则化方法:dropout(随机失活),也被誉为防止过拟合的大杀器.\n",
    "\n",
    "在2012年,Hinton在其论文[《Improving neural networks by preventing co-adaptation of feature detectors》](https://arxiv.org/pdf/1207.0580.pdf)中提出Dropout.\n",
    "\n",
    "在2012年,Alex,Hinton在其论文[《ImageNet Classification with Deep Convolutional Neural Networks》](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)中用到了Dropout算法,用于防止过拟合.\n",
    "\n",
    "论文地址:\n",
    "\n",
    "[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout实际上就是我们在前向传播的时候,让某个神经元的激活值以一定的概率$p$停止工作,这样可以使模型泛化性更强,因为它不会太依赖某些局部的特征,从而使得网络变成一个较小的神经网络.\n",
    "\n",
    "假设我们的standard NN如下图所示:\n",
    "\n",
    "<img src=\"../../picture/23.png\" width=500 heigth=\"500\">\n",
    "\n",
    "Dropout NN 如下图所示:\n",
    "\n",
    "<img src=\"../../picture/24.png\" width=500 heigth=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用第三层$(l = 3)$的网络来举例:\n",
    "\n",
    "(1) 首先随机(临时)删掉网络中的隐藏神经元,输入输出神经元保持不变.\n",
    "\n",
    "```python\n",
    "d3 = np.random.rand(a3.shape[0],a3.shape[1]) < keep_prob```\n",
    "\n",
    "- random.rand返回的是[0, 1)之间的数字\n",
    "\n",
    "- a3.shape[0],a3.shape[1],选择这样的一个shape是为了可以消除第三层上的单元节点,因为和a3的shape相同.\n",
    "\n",
    "- keep_prob:为保留率,比如keep_prob=0.5,就说明保留大约50%的神经元.\n",
    "\n",
    "从第三层中获取激活函数a3(包括激活函数计算步骤),a3 = np.multiply(a3,d3),其实就是a3 * d3(逐个元素相乘),它的作用就是过滤d3中所有等于0(false)的元素,乘法运算最终把a3中的相应元素归0.\n",
    "\n",
    "这样就可以实现随机删掉某些神经元,因为dropout后的结果为0,相当于这些神经元是无效操作."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 将dropout后的a3继续传入输出层.得到与$\\hat{y}$相同的形状.\n",
    "\n",
    "(3) 反向传播时,也需要将将进行da3 = np.multiply(da3,d3)操作,因为我们向后传播消失神经元节点要与向前传播的神经元节点相同.也就是说我们在更新参数的时候,**是不更新已经消除的神经元节点的权重与偏置.**\n",
    "\n",
    "(4) 对每一层都进行类似的操作,那么每次迭代实际上都是使用**不同的神经网络.**\n",
    "\n",
    "可以看到Dropout NN的最终结果与L2的结果非常相似,也是将某层消除神经元来达到预防过拟合."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout的操作:**\n",
    "\n",
    "<img src=\"../../picture/25.png\" width=500 heigth=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上对于Dropout的操作有两种方式:\n",
    "\n",
    "(0) 没有Dropout的网络计算公式:\n",
    "\n",
    "$Z^{[l]}=W^{[l]}y^{[l-1]}+b^{[l]}$\n",
    "\n",
    "$y^{[l]}=Activate(Z^{[l]})$\n",
    "\n",
    "(1) Dropout不做缩放处理(权重期望发生变化):\n",
    "\n",
    "$P\\sim Bernoulli(P)$\n",
    "\n",
    "$\\tilde{y}^{[l-1]}=P^{[l-1]}*y^{[l-1]}$\n",
    "\n",
    "$Z^{[l]}=W^{[l]}\\tilde{y}^{[l]}+b^{[l]}$\n",
    "\n",
    "$y^{[l]}=Activate(Z^{[l]})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$P\\sim Bernoulli(P)$就是随机生成0,1的向量,那么当$P^{[l-1]}*y^{[l-1]}$处理过后,就能使得某些神经元以概率$P$为0消失,从而达到防止过拟合的作用.\n",
    "\n",
    "实际上就相当于上面的\n",
    "\n",
    "```python\n",
    "d3 = np.random.rand(a3.shape[0],a3.shape[1]) < keep_prob```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外由于我们随机删除一些神经节点,会使得权重期望发生变化,由于我们没有做缩放处理,所以我们在预测的时候需要做以下步骤:\n",
    "\n",
    "在预测的时候\n",
    "\n",
    "<img src=\"../../picture/26.png\" width=500 heigth=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测阶段Dropout公式:\n",
    "\n",
    "$W_{test}^{[l]}=pW^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:orange\">实际上这里可以看出,预测是带有随机性的,因为存在随机$𝐵𝑒𝑟𝑛𝑜𝑢𝑙𝑙𝑖(𝑃)$,如果丢弃一些神经元,这会带来结果不稳定的问题.也就是说这个系统它是不稳定的,所以存在一种\"补偿\"的做法----进行缩放.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Dropout做缩放处理:\n",
    "\n",
    "$P\\sim Bernoulli(P)$\n",
    "\n",
    "$\\tilde{y}^{[l-1]}=P^{[l-1]}*y^{[l-1]}$\n",
    "\n",
    "对$y^{[l]}$ 进行缩放\n",
    "\n",
    "$\\tilde{y}^{[l]} = \\frac{\\tilde{y}^{[l]}}{P}$\n",
    "\n",
    "$Z^{[l]}=W^{[l]}\\tilde{y}^{[l]}+b^{[l]}$\n",
    "\n",
    "$y^{[l]}=Activate(Z^{[l]})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么如果做了缩放处理就无需在测试上做Dropout处理.\n",
    "\n",
    "另外在早期版本,都没有进行自除操作,所以在最后会让平均值变得越来越复杂.所以我们经常会使用缩放的方式,那么下面总结Dropout(缩放)的流程:\n",
    "\n",
    "**FP:**\n",
    "$d^{[l]} = np.random.rand(A^{[l]}.shape[0],A^{[l]}.shape[1]) < keep-pro$\n",
    "\n",
    "$A^{[l]} = np.multiply(A^{[l]},d^{[l]})$\n",
    "\n",
    "$A^{[l]} = \\frac{A^{[l]}}{keep-pro}$\n",
    "\n",
    "这里的keep-pro为保留率,也就是说相当于$1-P$,比如keep-pro=0.8,说明80%的节点保留,删除20%.\n",
    "\n",
    "(3) 代码示例:\n",
    "\n",
    "<img src=\"../../picture/63.png\" width=500 heigth=\"500\">\n",
    "\n",
    "**BP:**\n",
    "\n",
    "$dA^{[l]} = np.multiply(dA^{[l]},d^{[l]})$\n",
    "\n",
    "$dA^{[l]} = \\frac{dA^{[l]}}{keep-prob}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "\n",
    "(1)缩放可以使得其期望贴近于原期望\n",
    "\n",
    "比如下面这个例子\n",
    "\n",
    "$\\mu = \\frac{1+1+1+1}{4}=1$,那么我们随机消除两个($P=0.5$)即变为:\n",
    "\n",
    "$\\mu = \\frac{1+1}{4}=0.5$\n",
    "\n",
    "很显然我们的期望会发生变化,但是如果将其除上$1-P$:\n",
    "\n",
    "$\\mu = \\frac{\\frac{1+1}{4}}{0.5}=1$\n",
    "\n",
    "可以看到期望不会发生变化.\n",
    "\n",
    "(2) 我们一般不对输入层(原数据集),输出层(预测值)做Dropout处理.\n",
    "\n",
    "\n",
    "[PDF](../../PDFS/C2W1L06notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Understanding Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)取平均的作用:\n",
    "\n",
    "先回到标准的模型即没有dropout,我们用相同的训练数据去训练5个不同的神经网络,一般会得到5个不同的结果,此时我们可以采用 \"5个结果取均值\"或者\"多数取胜的投票策略\"去决定最终结果.例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9,其它两个网络给出了错误结果.这种\"综合起来取平均\"的策略通常可以有效防止过拟合问题.因为不同的网络可能产生不同的过拟合,取平均则有可能让一些\"相反的\"拟合互相抵消.dropout掉不同的隐藏神经元就类似在训练不同的网络,随机删掉某些隐藏神经元导致网络结构已经不同,整个dropout过程就相当于对很多个不同的神经网络取平均.而不同的网络产生不同的过拟合,一些互为\"反向\"的拟合相互抵消就可以达到整体上减少过拟合.\n",
    "\n",
    "(2)减少神经元之间复杂的共适应关系:\n",
    "\n",
    "因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现.这样权值的更新不再依赖于有固定关系的隐含节点的共同作用,阻止了某些特征仅仅在其它特定特征下才有效果的情况.迫使网络去学习更加鲁棒的特征,这些特征在其它的神经元的随机子集中也存在.换句话说假如我们的神经网络是在做出某种预测,它不应该对一些特定的线索片段太过敏感,即使丢失特定的线索,它也应该可以从众多其它线索中学习一些共同的特征.从这个角度看dropout就有点像L1,L2正则,减少权重使得网络对丢失特定神经元连接的鲁棒性提高.\n",
    "\n",
    "\n",
    "(3) 不同hidden layer可以使用不同的keep-pro\n",
    "\n",
    "<img src=\"../../picture/27.png\" width=500 heigth=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如上述神经网络,很明显hidden layer-1,hidden layer-2相比于其他层更容易发生过拟合,那么我们keep-pro应该比较低,而其他层可以保持较高的keep-pro.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "(1) 当前Dropout被大量利用于全连接网络,而且一般认为设置为0.5或者0.3,而在卷积网络隐藏层中由于卷积自身的稀疏化以及稀疏化的ReLu函数的大量使用等原因,Dropout策略在卷积网络隐藏层中使用较少.总体而言,Dropout是一个超参,需要根据具体的网络,具体的应用领域进行尝试.\n",
    "\n",
    "(2)实施dropout在computer vision 上应用广泛,因为CV上的输入量非常大,但是在其他领域就相对较少.并且如果是针对于卷积操作那么由于卷积本身就是在做稀疏处理,所以很少会加Dropout.\n",
    "\n",
    "(3) 除非算法过拟合,否则尽量不要使用正则化\n",
    "\n",
    "(4)dropout的一大缺点就是代价函数J不能再被明确定义(因为神经元随机删除),最起码是很难去定义,所以我们通常会关闭dropout函数,keep-prob = 1,来绘制cost function.也就是说在输出层我们不会采取Dropout.\n",
    "\n",
    "(5)dropout缺点:\n",
    "\n",
    "- 需要使用交叉验证,你要搜索更多的超级参数(因为神经单元有变化)\n",
    "\n",
    "   \n",
    "    \n",
    "Reference:\n",
    "\n",
    "来自于[深度学习中Dropout原理解析](https://zhuanlan.zhihu.com/p/38200980)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Other regularization methods\n",
    "\n",
    "实际上如果你发现模型过拟合,做好的方法还是增加数据集,那么如果在没有办法增加数据集的情况下我们可以:\n",
    "\n",
    "(1) 将图片翻转\n",
    "\n",
    "(2) 图片正确裁剪\n",
    "\n",
    "(3) 图片轻微变形\n",
    "\n",
    "...\n",
    "\n",
    "以(1),(2),(3)的方式加入到训练样本中,可以产生新的数据,虽然没有直接给予新样本的效果好,但是可能在一定程度上还是可以防止过拟合的产生.\n",
    "\n",
    "<img src=\"../../picture/28.png\" width=500 heigth=\"500\">\n",
    "<img src=\"../../picture/29.png\" width=500 heigth=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有另外一种方法就是**early stopping.**\n",
    "\n",
    "运行梯度下降时，我们希望成本函数和误差逐渐变小如下图所示:\n",
    "\n",
    "<img src=\"../../picture/30.png\" width=500 heigth=\"500\">\n",
    "\n",
    "在验证集误差(它可以是验证集上的分类误差,或验证集上的代价函数,逻辑损失和对数损失),你会发现验证集误差通常会先呈现下降趋势,然后在某个节点处开始上升.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../../picture/31.png\" width=500 heigth=\"500\">\n",
    "\n",
    "那么通过在训练过程中增加判断验证集的错误率从而找到early stopping点."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种方法是使用不同的optimizer,比如Momentum,RMSprop,Adam等,这些后面都会提到.使用不同的optimizer来抑制梯度.\n",
    "\n",
    "但是如果使用了early stopping,那么可能对optimizer就不会很友好,因为提早停止,会使得optimizer并没有更高的优化."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Normalizing inputs\n",
    "\n",
    "在训练神经网络的中一个加速训练的方法就是标准化(normal),一般都是针对于特征而言,也就是处理轴向是特征轴.也有情况是针对整体样本\n",
    "\n",
    "假设我们有一个训练集,它有两个输入特征$X=[x_1,x_2]$\n",
    "\n",
    "<img src=\"../../picture/32.png\" width=500 heigth=\"500\">\n",
    "\n",
    "normal的过程如下:\n",
    "\n",
    "那么对于$m$个样本而言,\n",
    "\n",
    "$\\mu = \\frac{\\sum_{i=1}^{m}x_i}{m}$\n",
    "\n",
    "$X = X- \\mu$\n",
    "\n",
    "经过这一步将会使得样本均值为0\n",
    "\n",
    "<img src=\"../../picture/33.png\" width=500 heigth=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下去\n",
    "\n",
    "$\\sigma^{2}=\\frac{\\sum_{i=1}^{m}x_i^{2}}{m}$\n",
    "\n",
    "$X =\\frac{X}{\\sigma^{2}}$\n",
    "\n",
    "经过这一步处理会使得样本方差为1\n",
    "<img src=\"../../picture/34.png\" width=500 heigth=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以整体步骤为:\n",
    "\n",
    "$\\mu = \\frac{\\sum_{i=1}^{m}x_i}{m}$\n",
    "\n",
    "$X = X- \\mu$\n",
    "\n",
    "$\\sigma^{2}=\\frac{\\sum_{i=1}^{m}x_i^{2}}{m}$\n",
    "\n",
    "$X =\\frac{X}{\\sigma^{2}}$\n",
    "\n",
    "normal过后将会使得样本变为均值为0,方差为1的情况.\n",
    "\n",
    "\n",
    "\n",
    "**那么为什么我们想标准化输入特征呢?**\n",
    "\n",
    "对于$cost\\;function\\;J(w,b) = \\frac{ \\sum_{i=1}^{m}( L(\\hat{y},y) )}{m}$\n",
    "\n",
    "考虑如下情况:\n",
    "\n",
    "假设两个特征向量$x_1=[1,1000],x_2=[0,1]$,那么产生的$w_1$和$w_2$将会差距非常大,那么整体代价函数会变成一个非常狭窄的形状\n",
    "\n",
    "\n",
    "<img src=\"../../picture/35.png\" width=500 heigth=\"500\">\n",
    "\n",
    "那么如果在这种狭窄的代价函数下进行梯度下降,会发现需要更长的步骤才能达到中心点\n",
    "\n",
    "<img src=\"../../picture/37.png\" width=500 heigth=\"500\">\n",
    "\n",
    "但是如果使用normal之后,数据量之间将不存在太大的差距,这代价函数的形状将会是类似于\"圆\".\n",
    "\n",
    "<img src=\"../../picture/38.png\" width=500 heigth=\"500\">\n",
    "\n",
    "如果在此中情况,梯度会下降的更快\n",
    "\n",
    "<img src=\"../../picture/39.png\" width=500 heigth=\"500\">\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "如果多个特征值相差的范围较大,那么标准化特征值就非常重要了,如果特征值处于相似范围内,那么标准化就不那么重要了,但是执行标准化也不会有什么危害,所以经常会将输入特征进行标准化."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "\n",
    "(1) 训练样本使用了normal那么测试样本同样也需要.\n",
    "\n",
    "(2) 实际上对于神经网络而言,真正有效的normal方式应该是Batch Normalization.\n",
    "\n",
    "[PDF](../../PDFS/C2W1L09notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Vanishing/exploding gradients \n",
    "\n",
    "梯度消失/梯度爆炸是神经网络中常常会出现的问题,也是很难避免的问题,特别是深层的神经网络结构,事实上为了避免梯度消失或者梯度爆炸,会有一些方法比如[LSTM](https://www.zhihu.com/question/34878706/answer/152555959),Batch Normalization,我们以后慢慢讲解,现在我们下来看看为什么会产生梯度消失或者梯度爆炸.\n",
    "\n",
    "\n",
    "\n",
    "比如对于下面这种结构:\n",
    "\n",
    "<img src=\"../../picture/40.png\" width=500 heigth=\"500\">\n",
    "\n",
    "假设正在训练这样的一个神经网络,为了方便我们将$g(z) = g^{[l]}(z) = z$,是一个线性函数,且b^[l] = 0.\n",
    "\n",
    "那么\n",
    "\n",
    "\n",
    "$z^{[1]} = w^{[1]}x$\n",
    "\n",
    "$a^{[1]} = g(z^{[1]}) = z^{[1]}$\n",
    "\n",
    "$a^{[2]} = g(z^{[2]}) = g(w^{[2]}a^{[1]}) = w^{[2]}z^{[1]} = w^{[2]} w^{[1]}x$\n",
    "\n",
    "$\\cdots$\n",
    "\n",
    "$\\hat{y} = w^{[L]}w^{[l-1]}.....w^{[2]}w^{[1]}x$\n",
    "\n",
    "(1)如果$w<1$可以看出最终的$\\hat{y}$将是一个非常小的值接近于0.\n",
    "\n",
    "我们这做输出层(sigmoid)考虑,实际上其他层无论使用什么激活函数,当神经网络深度够深的时候也是一样的结论:\n",
    "\n",
    "$w = w - \\alpha\\cdot (\\hat{y}-Y)$,可以看出最终梯度不会更新.于是会产生梯度消失.\n",
    "\n",
    "在比如我们对于hidden layers中的ReLu.\n",
    "\n",
    "$z = ReLu(wx)$,那么$z$也不会发生什么变化,导致$dw=dz\\cdot a$也不会发生任何变化.\n",
    "\n",
    "简单地说,根据链式法则,如果每一层神经元对上一层的输出的偏导乘上学习率结果都小于1的话,那么即使这个结果是0.99,在经过足够多层传播之后,误差对输入层的偏导会趋于0$(\\lim_{n\\to\\infty}0.99^n=0)$\n",
    "\n",
    "(2) 同理如果$w>1$,那么随着深度的增加$\\hat{y}$会呈现指数式的增长.于是就会产生梯度爆炸.\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "直观来看:\n",
    "\n",
    "(1) 梯度消失:在迭代过程中梯度不在更新,sigmoid函数而言,也就是处于图像中最左和最优的平缓地带,导数为0.\n",
    "\n",
    "(2) 梯度爆炸:在迭代过程中,梯度会呈现指数式的增长,最终会出现$\\infty$的形式.\n",
    "\n",
    "(3) 主要包括以下几种:\n",
    "\n",
    "(3.1) 神经网络权重初始化不当;比如sigmoid;\n",
    "\n",
    "(3.2) 激活函数选择不当;比如使用线性函数;\n",
    "\n",
    "(3.3) 网络结构本身的问题;比如RNN;\n",
    "\n",
    "(3.4) 数据集的问题,如标注等.\n",
    "\n",
    "(4) 更多查看[为什么会产生梯度消失?](https://www.zhihu.com/question/49812013)\n",
    "\n",
    "[PDF](../../PDFS/C2W1L10notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Weight initialization for deep networks\n",
    "\n",
    "对于梯度消失以及梯度爆炸,我们可以选择较好的初始化权重,虽然不能彻底解决问题,但是还是一个比较好的方案.\n",
    "\n",
    "对于sigmoid而言,直观上可以理解为初始化权重值不要在sigmoid两侧.\n",
    "\n",
    "(1) 选择使用Xavier initialization:\n",
    "\n",
    "$w^{[l]} = np.random.randn(shape) * np.sqrt(\\frac{1}{n^{[l-1]}})$其中$n$代表当前隐藏层神经元节点个数.\n",
    "\n",
    "$w^{[l]} = np.random.randn(shape) * np.sqrt(\\frac{2}{(n^{[l-1]}+n^{[l]})}) $\n",
    "\n",
    "$w^{[l]} = np.random.randn(shape) * np.sqrt(\\frac{6}{(n^{[l-1]}+n^{[l]})}) $\n",
    "\n",
    "(2) normal输出样本.\n",
    "\n",
    "那么在输入特征值时零均值,1方差的情况,那么$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$也会调整到相似的范围,虽然这没有解决梯度消失或者爆炸的问题,但是确实将其概率降低了,因为他给权重矩阵设置了合理的值,因为你也知道权值矩阵不能比1大很多,也不能比1小很多."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 Numerical approximation of gradient\n",
    "\n",
    "有时候可能是因为cost函数$J(\\theta)$本身定义错误会使得最后的梯度发生问题,最终导致梯度的方向错误,始终不能收敛,如果你有耐心与时间,你可以尝试使用**梯度检验**来检查每次的梯度值是否正确.\n",
    "\n",
    "梯度检验通常是近似解,同时求解速度很慢.\n",
    "\n",
    "<img src=\"../../picture/41.png\" width=500 heigth=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$\\epsilon=0.001$一般都是一个比较小的值\n",
    "\n",
    "对于$f(\\theta)=\\theta^{3},\\theta=1$\n",
    "\n",
    "那么梯度应该为$f^{'}(\\theta=1)=3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "较大三角形的宽高比值更接近于$\\theta$,可以尝试计算\n",
    "\n",
    "$\\frac{f(\\theta+\\epsilon) - f(\\theta-\\epsilon)}{2*\\epsilon}\\cong3.0001$\n",
    "\n",
    "而\n",
    "\n",
    "$\\frac{f(\\theta+\\epsilon) - f(\\theta-\\epsilon)}{\\epsilon}\\cong 3.0301$\n",
    "\n",
    "很明显大的三角形(两边斜率部分)会更加接近于小三角形(斜率)部分,所以我们会使用双边检查的方式去近似梯度.\n",
    "\n",
    "也就是说:\n",
    "\n",
    "用解析的方式计算$f(\\theta)$在$\\theta$点处的梯度值$g(\\theta)$,若$g(\\theta)$和$\\frac{d}{\\theta}J(\\theta)=d_{\\theta}$的值很接近的话(也就是说两者差值小于一个极小项),说明$j(\\theta)$的定义没有多大问题.\n",
    "\n",
    "计算两者之间的距离使用\n",
    "\n",
    "$\\frac{||g(\\theta) -d_{\\theta}||_{2} }{||g(\\theta)||_2 + ||d_{\\theta}||_2}$.\n",
    "\n",
    "其中$||\\cdot||_2$为L2范数(Frobenius norm)\n",
    "\n",
    "假设$\\epsilon$的值为$10^{-7}$:\n",
    "\n",
    "如果你发现方程的值大约是$10^{-7}$那么这是最好的结果,说明梯度没有多大问题\n",
    "\n",
    "如果你发现方程的值大约是$10^{-5}$那么就要小心了,可能需要检查确保没有一项误差过大,比如可能有一项误差过大.\n",
    "\n",
    "如果是$10^{-3}$,那么就要重新审查参数了,要开始debug所有$\\theta$了.\n",
    "\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "\n",
    "1.不要在训练集中使用梯度检验,它只适合于debug.\n",
    "\n",
    "2.如果算法的梯度检验失败,那么要检查所有项,也就是要找出到底是哪个θ导致的,使得两者相差甚远,比如说某层db或者dw,可以帮助你快速初略定位bug.\n",
    "\n",
    "3.要注意成本函数J的形式是否带有正则化(Frobenius)\n",
    "\n",
    "4.梯度检验不能与dropout同时使用,因为每次迭代dropout会随机消除隐藏单元,从而难以计算其代价函数,所以可以设置keep prob = 1.0.\n",
    "\n",
    "5.除非再没有办法的情况下,否则不要使用梯度检验,这是一个非常耗时的操作.\n",
    "\n",
    "\n",
    "[PDF](../../PDFS/C2W1L12notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 Loss function\n",
    "\n",
    "有时候根据损失函数的形状,我们肯能能够推测出一些关于学习率的信息(reference Stanford).\n",
    "\n",
    "<img src=\"../../picture/69.png\" width=500 heigth=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "左图显示不同学习率对于loss的影响,随着高学习率,他们将开始看起来更具指数性.较高的学习率会更快地减少损失,所以将loss将下降的更快,但是也会很有可能会呈现绿色线的情况,这是因为优化中存在太多的\"能量\",参数在混乱中反弹,无法在优化环境中找到一个好位置,从而导致loss总是在大致的一个范围内.\n",
    "\n",
    "右图为在CIFAR-10数据集上训练小型网络时,典型损失函数随时间变化的示例.这个损失函数看起来合理(它可能表示根据其衰减速度略微过小的学习率，但很难说),并且还表明批量大小可能有点太低(因为成本有点太高).\n",
    "\n",
    "对于批量大小,我们会在batch size中了解,一般情况下batch size越小,那么\"摆动\"的情况就会越明显,当批量为整体样本时,\"摆动\"最小,当批量为1时,\"摆动\"最大.\n",
    "\n",
    "这里还有一些[有趣的loss](lossfunctions.tumblr.com)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
