{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN(Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../picture/pytorch_CONV.png)\n",
    "\n",
    "这里我们使用Pytorch来实现Minist数据集的CNN网络,该网络模型与Tensorflow中的网络模型极为相似,但是不同点是我们没有在Pool层使用“SAME”,原因稍后解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "这里我们依然使用Keras的数据集,为了电脑(CPU)运行效率,我这里取training set 10K,validation set 0.1K.需要注意的是我们这里的样本shape已经发生变化,即:\n",
    "\n",
    "$data\\;shape:(m,Channels,Height,Width)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_data(index,limit_train,limit_test):\n",
    "    \"\"\"\n",
    "    Load data set.\n",
    "    Arguments:\n",
    "    ---------\n",
    "        index: show minist digits index.\n",
    "        limit_train: sample limit of training set,in this case, choose 10K.\n",
    "        limit_test: sample limit of validation set,in this case,choose 0,1K.\n",
    "    Returns:\n",
    "    -------\n",
    "        x_train: training data set. divide by 255.-->normal\n",
    "        x_test: testing data set. divide by 255.-->normal\n",
    "        y_train: training data labels. It's a hot vector, shape is (m,n_classes).\n",
    "        y_test: testing data labels,a hot vector,shape is (m,n_classes).\n",
    "    \"\"\"\n",
    "    (X_train,Y_train),(X_test,Y_test) = mnist.load_data()\n",
    "    x_train = X_train[:limit_train,...].reshape(-1,1,28,28)\n",
    "    x_test = X_test[:limit_test,...].reshape(-1,1,28,28)\n",
    "    y_train = Y_train[:limit_train]\n",
    "    y_test = Y_test[:limit_test]\n",
    "    \n",
    "    print('x_train reshape:\\n',x_train.shape)\n",
    "    print('x_test reshape:\\n',x_test.shape)\n",
    "    print('y_train shape:\\n ',y_train.shape)\n",
    "    print('y_test shape:\\n ',y_test.shape)\n",
    "    print('The Number is:{}'.format(Y_train[index]))\n",
    "    plt.imshow(X_train[index],cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    return x_train/255,x_test/255,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train reshape:\n",
      " (10000, 1, 28, 28)\n",
      "x_test reshape:\n",
      " (100, 1, 28, 28)\n",
      "y_train shape:\n",
      "  (10000,)\n",
      "y_test shape:\n",
      "  (100,)\n",
      "The Number is:5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = Load_data(0,10000,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到样本可以正常的加载出来,需要再次强调的是此时的$data\\;shape(m,Channels,Heights,Width)$,那么接下去我们来看看如何使用Pytorch搭建一个CNN网络."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pytorch of CNN Model\n",
    "\n",
    "在使用Pytorch构建CNN之前,我们要先来看看CNN中最重要的两个函数:\n",
    "\n",
    "1) [torch.nn.Conv2d](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d)\n",
    "\n",
    "```torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')```\n",
    "\n",
    "(1.1) in_channels:表示输入数据的通道数\n",
    "\n",
    "(1.2) out_channels: 表示输出数据的通道数其等于我们的卷积核数量.\n",
    "\n",
    "(1.3) kernel_size: 卷积核的大小,其可以是一个int类型(int=Heights=Widths),也可以是一个tuple(Heights,Widths)\n",
    "\n",
    "(1.3) stride: 卷积核的步长,其可以是一个int类型(int=Heights stride=Widths stride),也可以是一个tuple(Heights stride,Widths stride)\n",
    "\n",
    "(1.4) padding: 数据向外padding的层数,默认为0,其可以是一个int类型(int=Heights pad=Widths pad),也可以是一个tuple(Heights pad,Widths pad)\n",
    "\n",
    "(1.5) dilation: 卷积的空洞长度,这个参数目前为止我们是不需要理解的,我们在CNN(Theory)最后提到过这个空洞卷积的概念,具体情况查看[link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)\n",
    "\n",
    "(1.6) groups: 输入和输入的链接,我们目前也无需理会该参数,在之后我们会在与空洞卷积一起学习.\n",
    "\n",
    "(1.7) bias: 卷积层的偏置参数,默认为True.\n",
    "\n",
    "整体Pytorch Conv2d计算的过程为:\n",
    "\n",
    "对于输入$(N,C_{in},H,W)$经过卷积输出$(N,C_{out},H,W)$计算:\n",
    "\n",
    "<img src=\"../../picture/175.png\" height=500 width=500>\n",
    "\n",
    "其中 $\\star$表示互相关操作(也就是等价我们的\"卷积操作\"),两者之间相差一个卷积核的翻转.\n",
    "\n",
    "正式因为其这样的运算方式,我们需要将**Load Data**的形状更改为$data\\;shape(m,Channels,Heights,Width)$.\n",
    "\n",
    "**PS:**\n",
    "\n",
    "> 由于dilation的存在,所以我们的CONV输出结果更改为:\n",
    "\n",
    "> <img src=\"../../picture/174.png\" height=500 width=500>\n",
    "\n",
    "> 这种方式与我们之前计算CONV输出大小是等价的.\n",
    "\n",
    "> 观察类Conv2d可以发现,其是没有padding = \"SAME\"的功能的,也就是说我们需要手动定义padding=“SAME”的方法,依据其计算结果的方式,我们可以反推出如下形式:\n",
    "\n",
    "> $padding = \\frac{(H_{out}-1)*stride + 1 + dilation * (kernel_{size} -1)-H_{in}}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) [nn.MaxPool2d](https://pytorch.org/docs/stable/nn.html?highlight=max%20pool#torch.nn.MaxPool2d)\n",
    "\n",
    "```torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)```\n",
    "\n",
    "(2.1) kernel_size:卷积核的大小,其可以是一个int类型(int=Heights=Widths),也可以是一个tuple(Heights,Widths)\n",
    "\n",
    "(2.2) stride:卷积核的步长,其可以是一个int类型(int=Heights stride=Widths stride),也可以是一个tuple(Heights stride,Widths stride)\n",
    "\n",
    "(2.3) padding: 数据向外padding的层数,默认为0,其可以是一个int类型(int=Heights pad=Widths pad),也可以是一个tuple(Heights pad,Widths pad)\n",
    "\n",
    "> MaxPool的padding需要注意我们不可以向外pad太多的层数,具体理解为在TF模型中,我们的MaxPool会保证\"SAME\",那么对于(28,28)的输入,卷积核(2,2),步长(2,2),那么我们需要pad=14才能保证输出的大小为(28,28).不过可惜的是,在Pytorch中,该pad数量过大,导致无法padding.\n",
    "\n",
    "> 详情请看[github-pytorch](https://github.com/pytorch/pytorch/issues/3298),其中解释为:\n",
    "\n",
    "> <img src=\"../../picture/176.png\" height=500 width=500>\n",
    "\n",
    "> 大概意思是MaxPool的初始值为```-inf```,如果padding值太大,可能会在边界中有值.\n",
    "\n",
    "> Pytorch官方人员apaszke建议可以使用```torch.nn.functional.pad```进行手动填充.\n",
    "\n",
    "> 以上就是我们对于该模型为什么不在MaxPool层padding的原因,实际上我们也很少会在Pool上使用padding.\n",
    "\n",
    "(2.4) dilation:同Conv2d中的解释\n",
    "\n",
    "(2.5) return_indices:是否返回感受野的最大索引.\n",
    "\n",
    "(2.6) ceil_mode:当感受野不能完整滑过输入,那么将取floor(向下取整)的模式."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch of CNN\n",
    "\n",
    "懂得了CNN中最重要的两个函数,那么我们下面就可以开始构建CNN模型."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整体的模型架构为:\n",
    "\n",
    "[1] 卷积层:\n",
    "\n",
    "```python\n",
    "self.CONV = nn.ModuleDict({\n",
    "            'conv1':self.CONV2D((5,5,1,32),(1,1)),\n",
    "            'relu1':nn.ReLU(),\n",
    "            'pool1':self.POOL2D((2,2),(2,2),method_=\"VALID\"),\n",
    "            'conv2':self.CONV2D((5,5,32,64),(1,1)),\n",
    "            'relu2':nn.ReLU(),\n",
    "            'pool2':self.POOL2D((2,2),(2,2),method_=\"VALID\")\n",
    "        })```\n",
    "        \n",
    "对于```CONV2D(self,kernels,strides,dilation=(1,1),method_=\"SAME\"):```\n",
    "\n",
    "(1.1) 我们先获取```kernels```,```strides```,```method_```的参数值,如果```method_```是\"SAME\",那么我们就需要根据上述公式计算padding值.让后按照参数形式依次放入即可.需要注意的是,其参数只能接受为int类型.\n",
    "\n",
    "```python\n",
    "p_h = int(((self.n_h-1)*s_h+1+d_w*(f_h-1)-self.n_h) /2)\n",
    "p_w = int(((self.n_w-1)*s_w+1+d_h*(f_w-1)-self.n_w) /2)```\n",
    "\n",
    "(1.2) 接下去,我们需要更改```slef.n_h,self.n_w,self.n_c```,因为我们使用的类变量,经过conv后三者都会改变.我们需要重新赋值才能使得下面的Pool或者CONV成功运行,否则```slef.n_h,self.n_w,self.n_c```还是原来第一次CONV的参数,并不适合后面的CONV或POOl.也就是说,如果我们输入的是$(10000,1,28,28)$,经过$F=(5,5,1,32),S=1,padd=SAME$,那么输出的即为$(10000,32,28,28)$,那么我们就需要重新更新```slef.n_h=28,self.n_w=28,self.n_c=32```.\n",
    "\n",
    "```python\n",
    "# change n_h,n_w,n_c\n",
    "self.n_h = self.n_h\n",
    "self.n_w = self.n_w\n",
    "self.n_c = f_m```\n",
    "\n",
    "(1.2.1) 如果我们选择的模式是\"VALID\",那么我们需要重新计算输出大小并重新赋值\n",
    "\n",
    "```python\n",
    "# change n_h,n_w,n_c\n",
    "self.n_h = int((self.n_h + 2*p_h-d_h*(f_h-1)-1) / s_h + 1)\n",
    "self.n_w = int((self.n_w + 2*p_w-d_w*(f_w-1)-1) / s_w + 1)\n",
    "self.n_c = f_m```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.1] Pool层:\n",
    "\n",
    "```POOL2D(self,kernel_size,strides,dilation=(1,1),method_=\"SAME\")```:\n",
    "\n",
    "该函数整体思想是和CONV2D层是一样的,不同的是由于POOL并不会改变数据的$Channels$,所以我们并不需要重新赋值```self.n_c```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 全连接层:\n",
    "\n",
    "```python\n",
    "self.fully_connected = nn.ModuleDict({\n",
    "            'FC3':self.FC(3136,1024),\n",
    "            'relu3':nn.ReLU(),\n",
    "            'dropout3':self.DROPOUT(0.3),\n",
    "            'FC4':self.FC(1024,10)\n",
    "        })```\n",
    "        \n",
    "应为我们最后POOL2输出的大小为$(64,7,7)$,将其展平即为:3136作为输入,我们输出为1024.则该层就是FC3.\n",
    "\n",
    "接下去我们使用```Dropout rate=0.3```来确保大约30%的神经节点停止工作以防止模型过拟合.\n",
    "\n",
    "最后一层输入为$(1024)$,输出为$(10)$即可,但是需要注意的是,由于我们选择的损失函数为```nn.CrossEntropyLoss```,所以我们不能使用```torch.softmax()```激活函数.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forward(nn.Module):\n",
    "    \"\"\"\n",
    "    Build Pytorch class.\n",
    "    \"\"\"\n",
    "    def __init__(self,data_shape):\n",
    "        \"\"\"\n",
    "        Initialization class.\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "            data_shape: shape of data, the shape (n_c,n_h,n_w). to using CONV and Pool.\n",
    "                        In each layer, n_c,n_h,n_w maby changed.\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        self.n_c,self.n_h,self.n_w = data_shape\n",
    "        # CONV layers. \n",
    "        # CONV1-->RELU1-->POOL1-->CONV2-->RELU2-->POOL2.\n",
    "        self.CONV = nn.ModuleDict({\n",
    "            'conv1':self.CONV2D((5,5,1,32),(1,1)),\n",
    "            'relu1':nn.ReLU(),\n",
    "            'pool1':self.POOL2D((2,2),(2,2),method_=\"VALID\"),\n",
    "            'conv2':self.CONV2D((5,5,32,64),(1,1)),\n",
    "            'relu2':nn.ReLU(),\n",
    "            'pool2':self.POOL2D((2,2),(2,2),method_=\"VALID\")\n",
    "        })\n",
    "        # Fully Connected\n",
    "        # (3136,1024) -->dropout(0.3) --> (1024,10),notic,do not use activated function.\n",
    "        self.fully_connected = nn.ModuleDict({\n",
    "            'FC3':self.FC(3136,1024),\n",
    "            'relu3':nn.ReLU(),\n",
    "            'dropout3':self.DROPOUT(0.3),\n",
    "            'FC4':self.FC(1024,10)\n",
    "        })\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self,X):\n",
    "        \"\"\"\n",
    "        Build Model Forward Propagation\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "            X: training data set,the shape is (m,n_c,n_h,n_w).\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "            X: FC4 layer out, the shape is (m,n_classes). No activate function.\n",
    "        \"\"\"\n",
    "        X = self.CONV['conv1'](X)\n",
    "        X = self.CONV['relu1'](X)\n",
    "        X = self.CONV['pool1'](X)\n",
    "        \n",
    "        X = self.CONV['conv2'](X)\n",
    "        X = self.CONV['relu2'](X)\n",
    "        X = self.CONV['pool2'](X)\n",
    "        m,_,_,_,= X.size()\n",
    "        X = torch.reshape(X,(m,-1)) # flatten data ,the shape is (m,-1).-1 = n_c*n_w,n_h\n",
    "\n",
    "        X = self.fully_connected['FC3'](X)\n",
    "        X = self.fully_connected['relu3'](X)\n",
    "        X = self.fully_connected['dropout3'](X)\n",
    "        X = self.fully_connected['FC4'](X)\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    def CONV2D(self,kernels,strides,dilation=(1,1),method_=\"SAME\"):\n",
    "        \"\"\"\n",
    "        CONV layers\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "            kernels: convolution kernel,the shape is (f_h,f_w,f_c,f_m).\n",
    "            strides: convolution stride,the shape is (s_h,s_w).\n",
    "            dilation: dilation size\n",
    "            method_: shoose convolution method, default \"SAME\",can choose \"VALID\".\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "            conv2d_: convolution layer output value,the shape is (m,f_m,n_h,n_w).m: number of batch size.\n",
    "        \"\"\"\n",
    "        f_h,f_w,f_c,f_m = kernels\n",
    "        \n",
    "        s_h,s_w = strides\n",
    "        d_h,d_w = dilation\n",
    "        \n",
    "        if method_ == \"SAME\":\n",
    "            p_h = int(((self.n_h-1)*s_h+1+d_w*(f_h-1)-self.n_h) /2)\n",
    "            p_w = int(((self.n_w-1)*s_w+1+d_h*(f_w-1)-self.n_w) /2)\n",
    "            conv2d_ = nn.Conv2d(self.n_c,f_m,\n",
    "                                kernel_size=(f_h,f_w),\n",
    "                                stride=(s_h,s_w),\n",
    "                                padding=(p_h,p_w),\n",
    "                                dilation=(d_h,d_w))\n",
    "            \n",
    "            # change n_h,n_w,n_c\n",
    "            self.n_h = self.n_h\n",
    "            self.n_w = self.n_w\n",
    "            self.n_c = f_m\n",
    "            \n",
    "        elif method_ == \"VALID\":\n",
    "            p_h = 0\n",
    "            p_w = 0\n",
    "            conv2d_ = nn.Conv2d(self.n_c,\n",
    "                                f_m,\n",
    "                                kernel_size=(f_h,f_w),\n",
    "                                stride=(s_h,s_w),\n",
    "                                dilation=(d_h,d_w))\n",
    "            \n",
    "            # change n_h,n_w,n_c\n",
    "            self.n_h = int((self.n_h + 2*p_h-d_h*(f_h-1)-1) / s_h + 1)\n",
    "            self.n_w = int((self.n_w + 2*p_w-d_w*(f_w-1)-1) / s_w + 1)\n",
    "            self.n_c = f_m\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('valid method %s'%method)\n",
    "        \n",
    "        return conv2d_\n",
    "    \n",
    "    \n",
    "    def POOL2D(self,kernel_size,strides,dilation=(1,1),method_=\"SAME\"):\n",
    "        \"\"\"\n",
    "        MAX POOL layers.\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "            kernel_size: Pooling layer kernel size,the shape is (f_h,f_w).\n",
    "            strides: Pooling layer stride,the shape is (s_h,s_w).\n",
    "            dilation: dilation shape.\n",
    "            method_: Pooling method, can choose \"SAME\",\"VALID\",default \"SAME\".\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "            max_pool2d: maxed pool value,the shape is (m,f_m,n_h,n_w)\n",
    "        \"\"\"\n",
    "        f_h,f_w = kernel_size\n",
    "        s_h,s_w = strides\n",
    "        d_h,d_w = dilation\n",
    "        if method_ == \"SAME\":\n",
    "            p_h = int(((self.n_h-1)*s_h+1+d_h*(f_h-1)-self.n_h) /2)\n",
    "            p_w = int(((self.n_w-1)*s_w+1+d_w*(f_w-1)-self.n_w) /2)\n",
    "            max_pool2d = nn.MaxPool2d(kernel_size=(f_h,f_w),stride=strides,padding=(p_h,p_w),dilation=(d_h,d_w))\n",
    "            # change n_h,n_w,n_c\n",
    "            self.n_h = self.n_h\n",
    "            self.n_w = self.n_w\n",
    "            \n",
    "        elif method_ == \"VALID\":\n",
    "            p_h = 0\n",
    "            p_w = 0\n",
    "            max_pool2d = nn.MaxPool2d(kernel_size=(f_h,f_w),stride=strides,dilation=(d_h,d_w))\n",
    "            # change n_h,n_w,n_c\n",
    "            self.n_h = int((self.n_h + 2*p_h-d_h*(f_h-1)-1) / s_h + 1)\n",
    "            self.n_w = int((self.n_w + 2*p_w-d_w*(f_w-1)-1) / s_w + 1)\n",
    "        else: \n",
    "            raise ValueError('valid method %s'%method)\n",
    "            \n",
    "        return max_pool2d\n",
    "    \n",
    "    def FC(self,Input_shape,Output_shape):\n",
    "        \"\"\"\n",
    "        FULLY-CONNECT layer.caculate linear value.\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "            Input_shape: Input data shape,the shape is (m,n_c*n_w,n_h)\n",
    "            Output_shape: Output data shape,the shape is (m,n_C*n_w,n_h)\n",
    "        Returns:\n",
    "        -------\n",
    "            linear_: linear layer output value.\n",
    "        \"\"\"\n",
    "        linear_ = nn.Linear(Input_shape,Output_shape)\n",
    "        nn.init.xavier_normal_(linear_.weight)\n",
    "        return linear_\n",
    "    \n",
    "    \n",
    "    def DROPOUT(self,rate):\n",
    "        \"\"\"\n",
    "        Dropout layers.\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "            rate: dropout rate,the rate =1-keep_probb\n",
    "        Return:\n",
    "        ------\n",
    "            dropout_: dropout layer output value.\n",
    "        \"\"\"\n",
    "        dropout_ = nn.Dropout(p=rate)\n",
    "        return dropout_\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以来尝试一下输入数据$(100,1,28,28)$经过Forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_data = x_train[:100]\n",
    "    test_data_tensor = Variable(torch.FloatTensor(test_data)) # you need set type is float tensor.\n",
    "    data_shape = test_data.shape\n",
    "    model_test = Forward(data_shape[1:])\n",
    "    out = model_test.forward(test_data_tensor)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到输出的结果是正确的形状,接下去我们开始构建整个CNN模型架构."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Function\n",
    "\n",
    "现在我们开始搭建Score函数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Score(model,cost,data,labels,is_loss=False):\n",
    "    \"\"\"\n",
    "    Socre function.\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "       cost: cost function to caculate loss value.\n",
    "       data: score data, the shape is (m,n_c,n_h,n_w).\n",
    "       labels: score data labes. The shape is (m,)\n",
    "       is_loss: caculate loss value,if is equal True.\n",
    "    Returns:\n",
    "    -------\n",
    "        accuracy: correct rate in current epoch.\n",
    "        loss: loss value in currcet epoch,if is euqal True.\n",
    "    \"\"\"\n",
    "    model.eval() # set pytorch model is testing model.\n",
    "    out = model.forward(data)\n",
    "    predict = torch.argmax(torch.softmax(out,dim=1),dim=1)\n",
    "    correct = torch.eq(predict,labels).type(torch.FloatTensor)\n",
    "    accuracy = torch.mean(correct)\n",
    "    if is_loss:\n",
    "        loss = cost(out,labels)\n",
    "        return accuracy,loss\n",
    "    else:\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model\n",
    "\n",
    "现在我们可以开始搭建完整版本的CNN Model.与之前的Pytorch一样.\n",
    "\n",
    "首先我们创建用于Score函数的```data```以及```val_data```,由于其不需要计算反向渐变,所以我们使用```with torch.no_grad():```.\n",
    "\n",
    "第二,我们使用pytorch自带的batch划分方式:\n",
    "\n",
    "```python\n",
    "data_set = Data.TensorDataset(X,y)\n",
    "loder = Data.DataLoader(dataset=data_set,batch_size=BATCH_SIZE,shuffle=True,num_workers=2)```\n",
    "\n",
    "需要注意的是需要将```shuffle=True```以防止过拟合."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Model(data,labels,val_data,val_labels,epochs,lr,BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Build CNN Model.\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "        data: training data set. the shape is (m,n_c,n_h,n_w)\n",
    "        labels: training labels,the shape is (m,)\n",
    "        val_data: validation data set, the shape equal data shape.\n",
    "        val_labels: validation labels, the shape equal labels shape.\n",
    "        epochs: Number of iterate.\n",
    "        lr: learning rate.\n",
    "        BATCH_SIZE: batch size for each epoc training.\n",
    "    \"\"\"\n",
    "    m,n_c,n_h,n_w = data.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        data_tensor = Variable(torch.FloatTensor(data))\n",
    "        labels_tensor = Variable(torch.LongTensor(labels))\n",
    "        val_data_tensor = Variable(torch.FloatTensor(val_data))\n",
    "        val_labels_tensor = Variable(torch.LongTensor(val_labels))\n",
    "        \n",
    "    # build model.\n",
    "    model = Forward((n_c,n_h,n_w))\n",
    "    # set loss function, notic, the labels shape must be (m,),it unacceptable shape like (m,n_classes)!!.\n",
    "    cost = nn.CrossEntropyLoss()\n",
    "    # choose adam optimizer.\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    # create Variable.\n",
    "    X = Variable(torch.FloatTensor(data))\n",
    "    y = Variable(torch.LongTensor(labels))\n",
    "    # Spliting data set.It's return tuple(mini_x,mini_y)\n",
    "    data_set = Data.TensorDataset(X,y)\n",
    "    loder = Data.DataLoader(dataset=data_set,batch_size=BATCH_SIZE,shuffle=True,num_workers=2)\n",
    "    \n",
    "\n",
    "    # Start training \n",
    "    for epoch in range(epochs):\n",
    "        # set pytorch training model.\n",
    "        model.train()\n",
    "        for mini_x,mini_y in loder:\n",
    "            \n",
    "            out = model.forward(mini_x)\n",
    "            loss = cost(out,mini_y)\n",
    "            optimizer.zero_grad() # clear grad.\n",
    "            loss.backward() # start backward.\n",
    "            optimizer.step() # updating parameters.\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            \n",
    "            acc_train,loss_train = Score(model,cost,data_tensor,labels_tensor,is_loss=True)\n",
    "            acc_val,loss_val = Score(model,cost,val_data_tensor,val_labels_tensor,is_loss=True)\n",
    "            \n",
    "            print('[{}/{}] train acc:{:.4f},train loss:{:.4f},val acc:{:.4f},val loss:{:.4f}'.format(epoch+1,\n",
    "                                                                                                    epochs,\n",
    "                                                                                                    acc_train,\n",
    "                                                                                                    loss_train,\n",
    "                                                                                                    acc_val,\n",
    "                                                                                                    loss_val))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] train acc:0.8873,train loss:0.3945,val acc:0.9100,val loss:0.3459\n",
      "[2/10] train acc:0.9239,train loss:0.2610,val acc:0.9400,val loss:0.2245\n",
      "[3/10] train acc:0.9433,train loss:0.1977,val acc:0.9700,val loss:0.1539\n",
      "[4/10] train acc:0.9522,train loss:0.1662,val acc:0.9600,val loss:0.1314\n",
      "[5/10] train acc:0.9589,train loss:0.1422,val acc:0.9700,val loss:0.1102\n",
      "[6/10] train acc:0.9688,train loss:0.1110,val acc:0.9900,val loss:0.0655\n",
      "[7/10] train acc:0.9745,train loss:0.0926,val acc:0.9900,val loss:0.0569\n",
      "[8/10] train acc:0.9784,train loss:0.0805,val acc:0.9900,val loss:0.0718\n",
      "[9/10] train acc:0.9798,train loss:0.0709,val acc:0.9900,val loss:0.0465\n",
      "[10/10] train acc:0.9829,train loss:0.0638,val acc:0.9900,val loss:0.0441\n"
     ]
    }
   ],
   "source": [
    "CNN_Model(x_train,y_train,x_test,y_test,10,lr=1e-4,BATCH_SIZE=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到经过10个epochs,得到的结果training accuracy为0.9879,testing accuracy 为 1.0.\n",
    "\n",
    "Congratulations,你已经学会如何使用Pytorch搭建一个简单的CNN模型."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
